{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Data Ingestion from Scratch\n",
    "\n",
    "In this notebook, we build a data ingestion pipeline into a vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muaddib/.cache/pypoetry/virtualenvs/llama-index-rag-from-scratch-xZSvx2P--py3.10/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "import os\n",
    "\n",
    "pinecone.init(api_key=os.getenv(\"PINECONE_API_KEY\"), environment=\"gcp-starter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index for text-embedding-002\n",
    "pinecone.create_index(\"quickstart\", dimension=1536, metric=\"euclidean\")\n",
    "\n",
    "pinecone_index = pinecone.Index(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] drop contents in index\n",
    "# pinecone_index.delete(deleteAll=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Createa PineconeVectorStore\n",
    "    - Simple wrapper for LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build Ingestion Pipeline fromScratch\n",
    "  1. Load Data\n",
    "  2. Use a Text Splitter to Split Documents\n",
    "  3. Manually construct Nodes from Text Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-19 19:07:00--  https://arxiv.org/pdf/2307.09288.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 128.84.21.199\n",
      "Connecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13661300 (13M) [application/pdf]\n",
      "Saving to: ‘data/llama2.pdf’\n",
      "\n",
      "data/llama2.pdf     100%[===================>]  13.03M   730KB/s    in 13s     \n",
      "\n",
      "2023-09-19 19:07:13 (1.02 MB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index import download_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PyMuPDFReader = download_loader(\"PyMuPDFReader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. split text to smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.text_splitter import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SentenceSplitter(chunk_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = []\n",
    "\n",
    "# maintain relationship with source doc index to help include metadata in (3)\n",
    "doc_idxs = []\n",
    "\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    cur_text_chunks = text_splitter.split_text(doc.text)\n",
    "    \n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Manually construct nodes from chunks\n",
    "\n",
    "- convert each chunk into a TextNode -> LlamaIndex abstraction to store data & define metadata + relationships to other nodes\n",
    "- inject metaddata from doc into each node\n",
    "- implementation of `SimpleNodeParser`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import TextNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(text=text_chunk)\n",
    "    src_doc = documents[doc_idxs[idx]]\n",
    "\n",
    "    node.metadata = src_doc.metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_pages': 77, 'file_path': './data/llama2.pdf', 'source': '77'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_doc.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_pages: 77\n",
      "file_path: ./data/llama2.pdf\n",
      "source: 2\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "72\n",
      "A.6 Dataset Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "75\n",
      "A.7 Model Card\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "77\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(nodes[3].get_content(metadata_mode=\"json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional] 4. Extract Metadata from each Node\n",
    "We extract metadata from each Node using our Metadata extractors.\n",
    "\n",
    "This will add more metadata to each Node.\n",
    "\n",
    "\n",
    "`TitleExtractor` - go through first `nodes` nodes and use LLM to create a title for each node, then use LLM to summarize titles into a single title for the document.\n",
    "  - adds `document_title` to each node\n",
    "\n",
    "`QuestionsAnsweredExtractor` - \n",
    "  - adds `questions_this_excerpt_answers` to each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser.extractors import (\n",
    "    MetadataExtractor,\n",
    "    QuestionsAnsweredExtractor,\n",
    "    TitleExtractor,\n",
    ")\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "metadata_extractor = MetadataExtractor(\n",
    "    extractors=[\n",
    "        TitleExtractor(nodes=5, llm=llm),\n",
    "        QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
    "    ],\n",
    "    in_place=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c800bd06b74766b62bfcce7c5e9b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting questions:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nodes = metadata_extractor.process_nodes(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Generate Embeddings for each Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Excerpt from document]\n",
      "total_pages: 77\n",
      "file_path: ./data/llama2.pdf\n",
      "source: 2\n",
      "document_title: Developing and Evaluating Llama 2-Chat: Pretraining, Fine-tuning, Safety Evaluation, Red Teaming, Dataset Contamination, and Model Card\n",
      "questions_this_excerpt_can_answer: 1. What are the safety measures implemented in the pretraining and fine-tuning processes of Llama 2-Chat?\n",
      "2. How is reinforcement learning with human feedback used in the fine-tuning of Llama 2-Chat?\n",
      "3. What is the process of red teaming and how is it applied to evaluate the safety of Llama 2-Chat?\n",
      "Excerpt:\n",
      "-----\n",
      "Contents\n",
      "1\n",
      "Introduction\n",
      "3\n",
      "2\n",
      "Pretraining\n",
      "5\n",
      "2.1\n",
      "Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "5\n",
      "2.2\n",
      "Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "5\n",
      "2.3\n",
      "Llama 2 Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "7\n",
      "3\n",
      "Fine-tuning\n",
      "8\n",
      "3.1\n",
      "Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "9\n",
      "3.2\n",
      "Reinforcement Learning with Human Feedback (RLHF)\n",
      ". . . . . . . . . . . . . . . . . . . . .\n",
      "9\n",
      "3.3\n",
      "System Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "16\n",
      "3.4\n",
      "RLHF Results\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "17\n",
      "4\n",
      "Safety\n",
      "20\n",
      "4.1\n",
      "Safety in Pretraining\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "20\n",
      "4.2\n",
      "Safety Fine-Tuning\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "23\n",
      "4.3\n",
      "Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "28\n",
      "4.4\n",
      "Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "print(nodes[1].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    node.embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Load Nodes into Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620ed4adadf14b599137e5b1520b3836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['5b8a563e-b010-4034-8bb7-dca8f0f97d3c',\n",
       " 'bfad99bb-bcfb-46c3-b1d0-32b32fa34b0b',\n",
       " 'f65a8dd2-a280-4256-98e2-a74b65642dfd',\n",
       " 'bfaa3b35-4ef4-465b-8a71-15d0fb24e370',\n",
       " 'd11ca168-6ccc-47bd-9945-1cf2b7aa1397',\n",
       " '2270d5a9-1d4d-41f7-ad95-6125c4ffeff5',\n",
       " 'd93f09bc-82c5-4c6c-b754-8e3c7e9f6bb7',\n",
       " '8f6ebfa3-1833-430e-bb77-1217ee5668fc',\n",
       " '2937ab84-c966-42b1-a98e-9ec86e7d724c',\n",
       " 'a7fd58a9-4dac-440e-82d9-7c7cd542a56e',\n",
       " '152ab395-cfe2-49b5-a556-3e3333f72e2b',\n",
       " 'ea7bccf1-aff2-4cea-9737-c1c8d79de397',\n",
       " 'ed916a2c-5706-4a57-860b-81309de4c0af',\n",
       " 'f4743a32-326c-4b6b-ad1b-4acdfa294c06',\n",
       " '8ce59037-d034-4271-a2fd-08ae5569f6d3',\n",
       " 'a76070f6-dbcd-4e15-a50c-0cf853ce1c62',\n",
       " 'b5d737da-95e2-4f6a-921f-45c66b4e7fc0',\n",
       " '9e1add67-3bb9-4550-bcc6-d8743cd55832',\n",
       " 'c772c457-d93d-4411-89cc-4c95dacea958',\n",
       " '8dcfaacd-fb09-4174-93aa-c577c102317f',\n",
       " 'd45c452c-00f6-479e-a62f-aedfe52817c6',\n",
       " 'cdbdad2e-d73a-4620-b9bd-c77c4c63c999',\n",
       " 'a4250d06-96fb-48ad-8a8c-e71753c56c23',\n",
       " '95303a77-d6d1-4159-b817-d68beabad8c2',\n",
       " '69e5a158-883e-441d-9d36-c33db4671309',\n",
       " '864799e7-4f0f-4585-9f5c-c52e88f80c50',\n",
       " '32001700-af5a-4476-bcac-d3cd9036f7f6',\n",
       " 'fdc818f2-f07a-42bb-bb3d-401578cb6320',\n",
       " '3eff9d52-ac85-4462-bf4b-33319070da72',\n",
       " 'f13de058-8212-4a8c-b9de-2b1542a7837d',\n",
       " '8a7a1a5a-7ced-4c65-b9f2-9f2a76db8b93',\n",
       " 'cd628609-fe69-4a49-a9d6-a3b10ce64e48',\n",
       " '93bf0a56-eb0c-45b3-af58-c768216ff5c2',\n",
       " 'd801402e-1220-42f4-b4a2-63542e8f0b99',\n",
       " '9d657085-526f-4377-9ec1-361c62f5e6da',\n",
       " '346689a8-e76f-4c73-a64e-ebfba5227f41',\n",
       " 'b91169b5-ad42-4767-a09d-b2bd3ddc19f6',\n",
       " '9e2c7d9d-0780-492f-86b8-2b0db88e5218',\n",
       " 'ce996aa9-60e2-4f6a-91f6-7b75dfb96497',\n",
       " '6feb5065-6fee-40b6-9a1d-5bcb8ddf6233',\n",
       " '23777af2-1507-4dcb-806f-b6ce53bc8b65',\n",
       " '3f9e791c-4802-4713-a2eb-27e45ca0380f',\n",
       " '37f809f0-b5b6-4f22-aa3c-5cc5c59992fc',\n",
       " 'ee4a4fb0-4850-4f8c-839e-8e995ada84ce',\n",
       " '7408a0f2-b2b9-48d5-b7a6-67cc5891c333',\n",
       " '5b5accb9-279d-4cc0-a816-dd7286ee8721',\n",
       " '6b5d1286-fde9-44ae-ac86-d3084eb8ed27',\n",
       " '3edb510d-5b91-4796-8b37-54e98b4c85e4',\n",
       " '1bea85f4-32b7-4e2e-8f7b-89ec53848e0d',\n",
       " '143da277-b301-47b5-bbe0-5ad7841a1c9b',\n",
       " '30eae953-850b-4036-ae31-2c848ac0fa8f',\n",
       " 'b8fcc189-e08d-4d6c-8f14-162843f31f2f',\n",
       " '664da3e2-395b-4bf6-ade5-f00ad8dc7c64',\n",
       " '13f33ebf-47e8-49ba-9d60-6b03bc55c195',\n",
       " 'c38b411c-ba0f-40ea-8c11-9f0ac6a0b64a',\n",
       " '176dcba5-d5fc-4b74-9867-cfdeceb0556b',\n",
       " 'da40b3ae-414e-4b98-a480-6d9cb54d1321',\n",
       " 'bc19b76e-ecf0-454b-b268-f878a4fb5bc5',\n",
       " '5fb05e1c-efda-4fa0-8488-7dfb634b8d2e',\n",
       " '159d1b13-9bdf-41f9-84c0-d24e38e52dc3',\n",
       " '2634228e-7b3c-4447-ad34-8d64f8692dc5',\n",
       " '2e5ae267-74e9-443e-ba05-c997b0d44aa9',\n",
       " '601e3648-6557-4201-98c4-a3ec75595dca',\n",
       " 'edee6cd9-406b-4396-ad0d-f8e967b9c1cd',\n",
       " '1ac30659-f095-435b-9de8-6308328c396c',\n",
       " '76f099c2-29c3-4d6c-aaea-eb03c1ef523a',\n",
       " '86ae2279-50b9-4c6f-b2e8-d261ef12e723',\n",
       " '7276af8b-e3d0-4a27-95b2-e8c6e1621236',\n",
       " '4a12fe33-0ded-48d4-bbcc-042ecebeed8a',\n",
       " 'e412e64f-ff18-4dca-8f9a-8ac7669ececc',\n",
       " '05337990-0530-4ec7-b705-c51cd1d08727',\n",
       " '00141a88-9d8d-46fe-8c61-2854e4732132',\n",
       " 'af70c093-cade-43f3-9e6b-cb802ea9a4a8',\n",
       " '159719e0-947d-412b-9f8b-891515c49c41',\n",
       " 'fddf00f5-67bf-4663-a321-5a50304b0210',\n",
       " 'd8e0d6be-660d-4586-86e3-46aec737e14b',\n",
       " '6fad4a4d-9958-49cb-b982-85f99e8c64c3',\n",
       " 'e90272f6-2a10-46c3-93b2-4fb2defa220c',\n",
       " '0fd3a890-31df-4fb9-8a47-a47781f52fae',\n",
       " '08f3a638-2134-4b22-8b81-1135a3dcd388',\n",
       " '1357faa2-00d0-40ab-bbb4-4635d8611ad3',\n",
       " '54543796-404d-47ed-b457-a324a99799c4',\n",
       " 'b3f197ba-57ff-4cc4-894f-a0b53ac8e533',\n",
       " '0f1bea72-cd1f-4e02-af31-5f3f15788f3a',\n",
       " 'd24e4823-1d3a-4c09-a3dd-d05a1573dc1d',\n",
       " '5a74a9e3-dbe8-470b-9f43-4043c53a4394',\n",
       " 'f704deb7-c5a5-40d6-bbc0-44aef77a51de',\n",
       " 'b4b29d9d-a405-43ce-9dbc-59cf3c6b9405',\n",
       " 'fa14f458-48e2-432c-9d1e-15ae3118633e',\n",
       " 'c22b3d13-c58d-4f06-85a3-2e1c334c5192',\n",
       " '3ac2ebc6-e886-4fac-a7b0-c82bde613903',\n",
       " '3f051a29-32aa-4d99-a170-f25a5a0cadbd',\n",
       " 'bae971fd-ce33-41a6-8bc7-825ac9025c11',\n",
       " '12c89085-59d2-424b-a8fe-22da0dd09129',\n",
       " 'f8e5c640-dbff-4eaf-b251-2ee6ad00c378',\n",
       " '686f43eb-f29d-4a14-8827-6bbe3837fb50',\n",
       " '26448aee-2673-401f-b116-d4a383b89c17',\n",
       " '7e867fb5-3c5e-4f2e-b25d-d8460f61dfe6',\n",
       " '1153ed46-85a7-4493-b2fd-06ae747820b3',\n",
       " '3723c9ed-38c4-43b7-a8c5-357a9b61d50a',\n",
       " '051d5735-c0c6-4d12-bf5f-37d206535822',\n",
       " '695a6d1b-ce1b-4bed-878d-ccf91b3c1958',\n",
       " 'ec7f0c8b-ef8b-4815-bb48-40600857c7b9',\n",
       " '710be917-4341-44ce-9a26-9aed39088146',\n",
       " '28a2669e-d7e7-48d1-90e7-a5afff47f318',\n",
       " '910e0b2c-5358-4a15-acc3-185bebc61fbf',\n",
       " 'cf71b74e-a8d6-4ee4-b038-c3cacbbb4752',\n",
       " '28705075-a7c5-43ae-8694-8131da20dcc0',\n",
       " 'fd1fb68f-5fa5-40f3-a577-ccdffd37df61',\n",
       " '2542bad8-f899-4e3c-8772-6f5e25e31076']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve & Query from Vector Store\n",
    "\n",
    "- Here we use `VectoreStoreIndex` to speed up querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.storage import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key concepts for safety fine-tuning include supervised safety fine-tuning, safety RLHF (Reinforcement Learning from Human Feedback), and safety context distillation. \n",
      "\n",
      "Supervised safety fine-tuning involves gathering adversarial prompts and safe demonstrations to train the model to align with safety guidelines. This is done before RLHF and helps lay the foundation for high-quality human preference data annotation.\n",
      "\n",
      "Safety RLHF integrates safety into the general RLHF pipeline. It includes training a safety-specific reward model and gathering more challenging adversarial prompts for rejection sampling style fine-tuning and PPO (Proximal Policy Optimization) optimization.\n",
      "\n",
      "Safety context distillation involves generating safer model responses by prefixing a prompt with a safety preprompt and then fine-tuning the model on the safer responses without the preprompt. This distills the safety preprompt (context) into the model and allows the safety reward model to choose whether to use context distillation for each sample.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you tell me about the key concepts for safety finetuning?\"\n",
    "response = query_engine.query(query)\n",
    "\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-rag-from-scratch-xZSvx2P--py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
