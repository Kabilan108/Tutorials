{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we look at building the \"LLM Synthesis\" component of a RAG pipeline\n",
    "    - Given a set of retieved nodes, we will synthesize a response even if the retrieved context overflows the context window.\n",
    "\n",
    "- Strategies\n",
    "    - Create & refine\n",
    "    - Tree summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-20 16:15:25--  https://arxiv.org/pdf/2307.09288.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 128.84.21.199\n",
      "Connecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13661300 (13M) [application/pdf]\n",
      "Saving to: ‘data/llama2.pdf’\n",
      "\n",
      "data/llama2.pdf     100%[===================>]  13.03M  1.97MB/s    in 7.2s    \n",
      "\n",
      "2023-09-20 16:15:33 (1.82 MB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_hub.file.pymu_pdf.base import PyMuPDFReader\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use highlevel abstractions to \n",
    "  - ingest data into pinecone\n",
    "  - get a vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import PineconeVectorStore\n",
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "from llama_index.storage import StorageContext\n",
    "\n",
    "import pinecone\n",
    "import os\n",
    "\n",
    "pinecone.init(api_key=os.environ[\"PINECONE_API_KEY\"], environment=\"gcp-starter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions for text-embedding-ada-002\n",
    "pinecone.create_index(\"quickstart\", dimension=1536, metric=\"euclidean\")\n",
    "\n",
    "pinecone_index = pinecone.Index(\"quickstart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818d6d1abd9e471ab90930686cd039b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upserted vectors:   0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create vector store and index\n",
    "vector_store = PineconeVectorStore(pinecone_index)\n",
    "\n",
    "# NOTE: set chunk size of 1024\n",
    "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    service_context=service_context,\n",
    "    storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given example question, get a retrieved set of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Can you tell me about results from RLHF using both model-based and human-based evaluation?\"\n",
    "\n",
    "retrieved_nodes = retriever.retrieve(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Building Response Synthesis with LLMS\n",
    "  1. Try a simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try sythesizing response using a single input prompt + LLM call\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model=\"text-davinci-003\")\n",
    "\n",
    "qa_prompt = PromptTemplate(\"\"\"\n",
    "Context information is below.\n",
    "-------------------\n",
    "{context_str}\n",
    "-------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "Query: {query_str}\n",
    "Answer: \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(retrieved_nodes, query_str, qa_prompt, llm):\n",
    "    \"\"\"Generate response from retrieved nodes and query string.\"\"\"\n",
    "\n",
    "    context_str = \"\\n\\n\".join([n.get_content() for n in retrieved_nodes])\n",
    "\n",
    "    fmt_qa_prompt = qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "    response = llm.complete(fmt_qa_prompt)\n",
    "\n",
    "    return str(response), fmt_qa_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Response******:\n",
      "RLHF results were evaluated using both model-based and human-based evaluation. Model-based evaluation was used to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5. Human evaluation was used to validate major model versions and measure the robustness of the reward model. Additionally, a general reward was trained to ensure the measure would not diverge from human preferences. Results showed that the reward models were well calibrated with human preference annotations. Furthermore, the model-based evaluation revealed that the temperature of the model was influenced by RLHF and could be dynamically re-scaled contingent upon the context.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a query\n",
    "query_str = \"Can you tell me about results from RLHF using both model-based and human-based evaluation?\"\n",
    "\n",
    "# retrieve nodes\n",
    "retrieved_nodes = retriever.retrieve(query_str)\n",
    "\n",
    "# generate response\n",
    "response, fmt_qa_prompt = generate_response(retrieved_nodes, query_str, qa_prompt, llm)\n",
    "print(f\"*****Response******:\\n{response}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Formatted Prompt*****:\n",
      "\n",
      "Context information is below.\n",
      "-------------------\n",
      "3.4\n",
      "RLHF Results\n",
      "3.4.1\n",
      "Model-Based Evaluation\n",
      "Evaluating LLMs is a challenging open-research problem. Human evaluation, while a gold standard, can\n",
      "be complicated by various HCI considerations (Clark et al., 2021; Gehrmann et al., 2023), and is not always\n",
      "scalable. Thus, to select the best-performing models among several ablations at each iteration from RLHF-V1\n",
      "to V5, we first observed the improvement of the rewards from the latest reward models, to save costs and\n",
      "increase iteration speed. We later validated major model versions with human evaluations.\n",
      "How Far Can Model-Based Evaluation Go?\n",
      "To measure the robustness of our reward model, we collected\n",
      "a test set of prompts for both helpfulness and safety, and asked three annotators to judge the quality of the\n",
      "answers based on a 7-point Likert scale (the higher the better). We observe that our reward models overall\n",
      "are well calibrated with our human preference annotations, as illustrated in Figure 29 in the appendix. This\n",
      "confirms the relevance of using our reward as a point-wise metric, despite being trained with a Pairwise\n",
      "Ranking Loss.\n",
      "Still, as Goodhart’s Law states, when a measure becomes a target, it ceases to be a good measure. To ensure\n",
      "our measure won’t diverge from the human preferences, we additionally used a more general reward, trained\n",
      "17\n",
      "\n",
      "5\n",
      "Discussion\n",
      "Here, we discuss the interesting properties we have observed with RLHF (Section 5.1). We then discuss the\n",
      "limitations of Llama 2-Chat (Section 5.2). Lastly, we present our strategy for responsibly releasing these\n",
      "models (Section 5.3).\n",
      "5.1\n",
      "Learnings and Observations\n",
      "Our tuning process revealed several interesting results, such as Llama 2-Chat’s abilities to temporally\n",
      "organize its knowledge, or to call APIs for external tools.\n",
      "SFT (Mix)\n",
      "SFT (Annotation)\n",
      "RLHF (V1)\n",
      "0.0\n",
      "0.2\n",
      "0.4\n",
      "0.6\n",
      "0.8\n",
      "1.0\n",
      "Reward Model Score\n",
      "RLHF (V2)\n",
      "Figure 20: Distribution shift for progressive versions of Llama 2-Chat, from SFT models towards RLHF.\n",
      "Beyond Human Supervision.\n",
      "At the outset of the project, many among us expressed a preference for\n",
      "supervised annotation, attracted by its denser signal. Meanwhile reinforcement learning, known for its insta-\n",
      "bility, seemed a somewhat shadowy field for those in the NLP research community. However, reinforcement\n",
      "learning proved highly effective, particularly given its cost and time effectiveness. Our findings underscore\n",
      "that the crucial determinant of RLHF’s success lies in the synergy it fosters between humans and LLMs\n",
      "throughout the annotation process.\n",
      "Even with proficient annotators, each individual writes with significant variation. A model fine-tuned on\n",
      "SFT annotation learns this diversity, including, unfortunately, the tail-end of poorly executed annotation. Fur-\n",
      "thermore, the model’s performance is capped by the writing abilities of the most skilled annotators. Human\n",
      "annotators are arguably less subject to discrepancy when comparing two outputs’ preference annotation\n",
      "for RLHF. Consequently, the reward mechanism swiftly learns to assign low scores to undesirable tail-end\n",
      "distribution and aligns towards the human preference. This phenomena is illustrated in Figure 20, where we\n",
      "can see that the worst answers are progressively removed, shifting the distribution to the right.\n",
      "In addition, during annotation, the model has the potential to venture into writing trajectories that even the\n",
      "best annotators may not chart. Nonetheless, humans can still provide valuable feedback when comparing two\n",
      "answers, beyond their own writing competencies. Drawing a parallel, while we may not all be accomplished\n",
      "artists, our ability to appreciate and critique art remains intact. We posit that the superior writing abilities of\n",
      "LLMs, as manifested in surpassing human annotators in certain tasks, are fundamentally driven by RLHF, as\n",
      "documented in Gilardi et al. (2023) and Huang et al. (2023). Supervised data may no longer be the gold\n",
      "standard, and this evolving circumstance compels a re-evaluation of the concept of “supervision.”\n",
      "In-Context Temperature Rescaling.\n",
      "We have observed an intriguing phenomenon related to RLHF, a feature\n",
      "not previously reported to the best of our knowledge: the dynamic re-scaling of temperature contingent upon\n",
      "the context. As indicated in Figure 8, the temperature appears to be influenced by RLHF. Yet, intriguingly,\n",
      "our findings also revealed that the shifts are not uniformly applied across all prompts, as shown in Figure 21.\n",
      "For instance, when it comes to prompts associated with creativity, such as “Write a poem,” an increase in\n",
      "temperature continues to generate diversity across our various RLHF iterations. This can be observed in the\n",
      "Self-BLEU slope, which mirrors a pattern comparable to that of the SFT model.\n",
      "On the other hand, for prompts based on factual information, such as “What is the capital of ?” the Self-BLEU\n",
      "slope diminishes over time. This pattern suggests that despite the rising temperature, the model learns to\n",
      "consistently provide the same response to factual prompts.\n",
      "32\n",
      "-------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: Can you tell me about results from RLHF using both model-based and human-based evaluation?\n",
      "Answer: \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"*****Formatted Prompt*****:\\n{fmt_qa_prompt}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Problem** - if we set the top-k retriever to a higher value? The context would overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The prompt is too long for the model. Please use a prompt that is less than 4097 tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/sietch/tutorials/llm/RAG/4_response_synthesis.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/4_response_synthesis.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m retriever \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39mas_retriever(similarity_top_k\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/4_response_synthesis.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m retrieved_nodes \u001b[39m=\u001b[39m retriever\u001b[39m.\u001b[39mretrieve(query_str)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/4_response_synthesis.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m response, fmt_qa_prompt \u001b[39m=\u001b[39m generate_response(retrieved_nodes, query_str, qa_prompt, llm)\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/4_response_synthesis.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResponse (k=5): \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/mnt/sietch/tutorials/llm/RAG/4_response_synthesis.ipynb Cell 18\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/4_response_synthesis.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m context_str \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([n\u001b[39m.\u001b[39mget_content() \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m retrieved_nodes])\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/4_response_synthesis.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m fmt_qa_prompt \u001b[39m=\u001b[39m qa_prompt\u001b[39m.\u001b[39mformat(context_str\u001b[39m=\u001b[39mcontext_str, query_str\u001b[39m=\u001b[39mquery_str)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/4_response_synthesis.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m response \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mcomplete(fmt_qa_prompt)\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/4_response_synthesis.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(response), fmt_qa_prompt\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llama-index-rag-from-scratch-xZSvx2P--py3.10/lib/python3.10/site-packages/llama_index/llms/base.py:277\u001b[0m, in \u001b[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mwith\u001b[39;00m wrapper_logic(_self) \u001b[39mas\u001b[39;00m callback_manager:\n\u001b[1;32m    268\u001b[0m     event_id \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_event_start(\n\u001b[1;32m    269\u001b[0m         CBEventType\u001b[39m.\u001b[39mLLM,\n\u001b[1;32m    270\u001b[0m         payload\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m         },\n\u001b[1;32m    275\u001b[0m     )\n\u001b[0;32m--> 277\u001b[0m     f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    278\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    279\u001b[0m         \u001b[39m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_gen\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CompletionResponseGen:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llama-index-rag-from-scratch-xZSvx2P--py3.10/lib/python3.10/site-packages/llama_index/llms/openai.py:144\u001b[0m, in \u001b[0;36mOpenAI.complete\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     complete_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_complete\n\u001b[0;32m--> 144\u001b[0m \u001b[39mreturn\u001b[39;00m complete_fn(prompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llama-index-rag-from-scratch-xZSvx2P--py3.10/lib/python3.10/site-packages/llama_index/llms/openai.py:281\u001b[0m, in \u001b[0;36mOpenAI._complete\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m all_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_all_kwargs(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tokens \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m     \u001b[39m# NOTE: non-chat completion endpoint requires max_tokens to be set\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     max_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_max_token_for_prompt(prompt)\n\u001b[1;32m    282\u001b[0m     all_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmax_tokens\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m max_tokens\n\u001b[1;32m    284\u001b[0m response \u001b[39m=\u001b[39m completion_with_retry(\n\u001b[1;32m    285\u001b[0m     is_chat_model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_chat_model,\n\u001b[1;32m    286\u001b[0m     max_retries\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mall_kwargs,\n\u001b[1;32m    290\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llama-index-rag-from-scratch-xZSvx2P--py3.10/lib/python3.10/site-packages/llama_index/llms/openai.py:343\u001b[0m, in \u001b[0;36mOpenAI._get_max_token_for_prompt\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    341\u001b[0m max_token \u001b[39m=\u001b[39m context_window \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(tokens)\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m max_token \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 343\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    344\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe prompt is too long for the model. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    345\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPlease use a prompt that is less than \u001b[39m\u001b[39m{\u001b[39;00mcontext_window\u001b[39m}\u001b[39;00m\u001b[39m tokens.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    347\u001b[0m \u001b[39mreturn\u001b[39;00m max_token\n",
      "\u001b[0;31mValueError\u001b[0m: The prompt is too long for the model. Please use a prompt that is less than 4097 tokens."
     ]
    }
   ],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=6)\n",
    "retrieved_nodes = retriever.retrieve(query_str)\n",
    "\n",
    "response, fmt_qa_prompt = generate_response(retrieved_nodes, query_str, qa_prompt, llm)\n",
    "print(f\"Response (k=5): {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  2. \"Create and Refine\" Strategy\n",
    "     - To deal with context overflows, we can synthesize a response sequentially through\n",
    "       all nodes.\n",
    "     - Start with first node, generate initial response\n",
    "     - For subsequent nodes, refine the answer using additional context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_prompt = PromptTemplate(\"\"\"\\\n",
    "The original query is as follows: {query_str}\n",
    "We have provided an existing answer: {existing_answer}\n",
    "We have the opportunity to refine the existing answer \\\n",
    "(only if needed) with some more context below.\n",
    "-------------------\n",
    "{context_str}\n",
    "-------------------\n",
    "Given the new context, refine the original answer to better answer the query. \\\n",
    "If the context isn't useful, return the original answer.\n",
    "Refined answer: \\\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response.notebook_utils import display_source_node\n",
    "\n",
    "def generate_response_cr(retrieved_nodes, query_str, qa_prompt, refine_prompt, llm):\n",
    "    \"\"\"Generate a response using create and refine strategy.\n",
    "    \n",
    "    The first node uses the 'QA' prompt.\n",
    "    All subsequent nodes use the 'refine' prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    cur_response = None\n",
    "    fmt_prompts = []\n",
    "\n",
    "    for idx, node in enumerate(retrieved_nodes):\n",
    "        print(f\"[Node {idx}]\")\n",
    "        display_source_node(node, source_length=2000)\n",
    "\n",
    "        context_str = node.get_content()\n",
    "\n",
    "        if idx == 0:\n",
    "            fmt_prompt = qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "        else:\n",
    "            fmt_prompt = refine_prompt.format(\n",
    "                context_str=context_str,\n",
    "                query_str=query_str,\n",
    "                existing_answer=cur_response\n",
    "            )\n",
    "        \n",
    "        cur_response = llm.complete(fmt_prompt)\n",
    "        fmt_prompts.append(fmt_prompt)\n",
    "\n",
    "    return str(cur_response), fmt_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node 0]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** b84c8932-df30-4e46-87f8-e7c59fc5fa13<br>**Similarity:** 0.210552812<br>**Text:** 3.4\n",
       "RLHF Results\n",
       "3.4.1\n",
       "Model-Based Evaluation\n",
       "Evaluating LLMs is a challenging open-research problem. Human evaluation, while a gold standard, can\n",
       "be complicated by various HCI considerations (Clark et al., 2021; Gehrmann et al., 2023), and is not always\n",
       "scalable. Thus, to select the best-performing models among several ablations at each iteration from RLHF-V1\n",
       "to V5, we first observed the improvement of the rewards from the latest reward models, to save costs and\n",
       "increase iteration speed. We later validated major model versions with human evaluations.\n",
       "How Far Can Model-Based Evaluation Go?\n",
       "To measure the robustness of our reward model, we collected\n",
       "a test set of prompts for both helpfulness and safety, and asked three annotators to judge the quality of the\n",
       "answers based on a 7-point Likert scale (the higher the better). We observe that our reward models overall\n",
       "are well calibrated with our human preference annotations, as illustrated in Figure 29 in the appendix. This\n",
       "confirms the relevance of using our reward as a point-wise metric, despite being trained with a Pairwise\n",
       "Ranking Loss.\n",
       "Still, as Goodhart’s Law states, when a measure becomes a target, it ceases to be a good measure. To ensure\n",
       "our measure won’t diverge from the human preferences, we additionally used a more general reward, trained\n",
       "17<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node 1]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 736c9643-6418-41e4-b40f-8d6d54803165<br>**Similarity:** 0.284444094<br>**Text:** 5\n",
       "Discussion\n",
       "Here, we discuss the interesting properties we have observed with RLHF (Section 5.1). We then discuss the\n",
       "limitations of Llama 2-Chat (Section 5.2). Lastly, we present our strategy for responsibly releasing these\n",
       "models (Section 5.3).\n",
       "5.1\n",
       "Learnings and Observations\n",
       "Our tuning process revealed several interesting results, such as Llama 2-Chat’s abilities to temporally\n",
       "organize its knowledge, or to call APIs for external tools.\n",
       "SFT (Mix)\n",
       "SFT (Annotation)\n",
       "RLHF (V1)\n",
       "0.0\n",
       "0.2\n",
       "0.4\n",
       "0.6\n",
       "0.8\n",
       "1.0\n",
       "Reward Model Score\n",
       "RLHF (V2)\n",
       "Figure 20: Distribution shift for progressive versions of Llama 2-Chat, from SFT models towards RLHF.\n",
       "Beyond Human Supervision.\n",
       "At the outset of the project, many among us expressed a preference for\n",
       "supervised annotation, attracted by its denser signal. Meanwhile reinforcement learning, known for its insta-\n",
       "bility, seemed a somewhat shadowy field for those in the NLP research community. However, reinforcement\n",
       "learning proved highly effective, particularly given its cost and time effectiveness. Our findings underscore\n",
       "that the crucial determinant of RLHF’s success lies in the synergy it fosters between humans and LLMs\n",
       "throughout the annotation process.\n",
       "Even with proficient annotators, each individual writes with significant variation. A model fine-tuned on\n",
       "SFT annotation learns this diversity, including, unfortunately, the tail-end of poorly executed annotation. Fur-\n",
       "thermore, the model’s performance is capped by the writing abilities of the most skilled annotators. Human\n",
       "annotators are arguably less subject to discrepancy when comparing two outputs’ preference annotation\n",
       "for RLHF. Consequently, the reward mechanism swiftly learns to assign low scores to undesirable tail-end\n",
       "distribution and aligns towards the human preference. This phenomena is illustrated in Figure 20, where we\n",
       "can see that the worst answers are progressively removed, shifting the distribution to the right.\n",
       "In addition, during annotation, the model has the potential to ven...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node 2]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 354e521d-a768-47f4-8d30-d0fc55b4a039<br>**Similarity:** 0.313639402<br>**Text:** RLHF-v5\n",
       "(with PPO)\n",
       "RLHF-v5\n",
       "(no PPO)\n",
       "RLHF-v4\n",
       "RLHF-v3\n",
       "            RLHF-v2\n",
       "      RLHF-v1\n",
       "SFT-v2       \n",
       "SFT-v1\n",
       "10%\n",
       "20%\n",
       "30%\n",
       "40%\n",
       "50%\n",
       "60%\n",
       "70%\n",
       "80%\n",
       "90%\n",
       "10%\n",
       "20%\n",
       "30%\n",
       "40%\n",
       "50%\n",
       "60%\n",
       "70%\n",
       "80%\n",
       "Helpfulness\n",
       "Judge: Meta Reward Models\n",
       "Harmlessness\n",
       "  RLHF-v5\n",
       "  (with PPO)\n",
       "RLHF-v5  \n",
       "(no PPO)  \n",
       "RLHF-v4\n",
       "RLHF-v3\n",
       "     RLHF-v2\n",
       "RLHF-v1     \n",
       "SFT-v2    \n",
       "SFT-v1\n",
       "10%\n",
       "20%\n",
       "30%\n",
       "40%\n",
       "50%\n",
       "60%\n",
       "70%\n",
       "80%\n",
       "90%\n",
       "10%\n",
       "20%\n",
       "30%\n",
       "40%\n",
       "50%\n",
       "60%\n",
       "70%\n",
       "80%\n",
       "Helpfulness\n",
       "Judge: GPT-4\n",
       "Harmlessness\n",
       "Figure 11: Evolution of Llama 2-Chat. We show the evolution after multiple iterations fine-tuning for the\n",
       "win-rate % of Llama 2-Chat compared to ChatGPT. Left: the judge is our reward model, which may favor\n",
       "our model, and right, the judge is GPT-4, which should be more neutral.\n",
       "on diverse open-source Reward Modeling datasets. We have not yet observed any such divergence, and\n",
       "hypothesize that iterative model updates may be helping to prevent this.\n",
       "As a last verification step to ensure no regression between our new model and the previous one, we use both\n",
       "to sample during the next annotation iteration. This enables a model comparison “for free” on new prompts\n",
       "and can help to increase diversity when sampling.\n",
       "Progression of Models.\n",
       "Figure 11 reports the progress of our different SFT and then RLHF versions for\n",
       "both Safety and Helpfulness axes, measured by our in-house Safety and Helpfulness reward models. On\n",
       "this set of evaluations, we outperform ChatGPT on both axes after RLHF-V3 (harmlessness and helpfulness\n",
       ">50%). Despite the aforementioned relevance of using our reward as a point-wise metric, it can arguably be\n",
       "biased in favor of Llama 2-Chat. Therefore, for a fair comparison, we additionally compute the final results\n",
       "using GPT-4 to assess which generation is preferred. The order in which ChatGPT and Llama 2-Chat outputs\n",
       "appeared in GPT-4 prompt are randomly swapped to avoid any bias. As expected, the win-rate in favor of\n",
       "Llama 2-Chat is less pronounced, although obtaining more than a 60% win-rate for our latest Llama 2-Chat.\n",
       "The prompt...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node 3]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 50c58113-4184-457b-8831-7cab0cce8e6a<br>**Similarity:** 0.332381606<br>**Text:** sampled human preferences, whereby human annotators select which of two model outputs they prefer.\n",
       "This human feedback is subsequently used to train a reward model, which learns patterns in the preferences\n",
       "of the human annotators and can then automate preference decisions.\n",
       "3.2.1\n",
       "Human Preference Data Collection\n",
       "Next, we collect human preference data for reward modeling. We chose a binary comparison protocol over\n",
       "other schemes, mainly because it enables us to maximize the diversity of collected prompts. Still, other\n",
       "strategies are worth considering, which we leave for future work.\n",
       "Our annotation procedure proceeds as follows. We ask annotators to first write a prompt, then choose\n",
       "between two sampled model responses, based on provided criteria. In order to maximize the diversity, the\n",
       "two responses to a given prompt are sampled from two different model variants, and varying the temperature\n",
       "hyper-parameter. In addition to giving participants a forced choice, we also ask annotators to label the degree\n",
       "to which they prefer their chosen response over the alternative: either their choice is significantly better, better,\n",
       "slightly better, or negligibly better/ unsure.\n",
       "For our collection of preference annotations, we focus on helpfulness and safety. Helpfulness refers to how\n",
       "well Llama 2-Chat responses fulfill users’ requests and provide requested information; safety refers to\n",
       "whether Llama 2-Chat’s responses are unsafe, e.g., “giving detailed instructions on making a bomb” could\n",
       "be considered helpful but is unsafe according to our safety guidelines. Separating the two allows us to\n",
       "apply specific guidelines to each and better guide annotators; for example, our safety annotations provide\n",
       "instructions to focus on adversarial prompts, among other guidance.\n",
       "Apart from differences in annotation guidelines, we additionally collect a safety label during the safety stage.\n",
       "This additional information bins model responses into one of three categories: 1) the preferred response\n",
       "is saf...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node 4]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** b24145d3-5c55-4bf3-a91f-3654468b84dd<br>**Similarity:** 0.336259484<br>**Text:** 1\n",
       "2\n",
       "3\n",
       "4\n",
       "5\n",
       "6\n",
       "7\n",
       "8\n",
       "9\n",
       "10\n",
       "11\n",
       "12\n",
       "13\n",
       "14\n",
       "Meta Helpfulness Data Batch Stage\n",
       "0.52\n",
       "0.54\n",
       "0.56\n",
       "0.58\n",
       "0.60\n",
       "0.62\n",
       "0.64\n",
       "Accuracy On All Examples\n",
       "7b\n",
       "13b\n",
       "70b\n",
       "GPT4\n",
       "OpenAssistant\n",
       "1\n",
       "2\n",
       "3\n",
       "4\n",
       "5\n",
       "6\n",
       "7\n",
       "8\n",
       "9\n",
       "10\n",
       "11\n",
       "12\n",
       "13\n",
       "14\n",
       "Meta Helpfulness Data Batch Stage\n",
       "0.50\n",
       "0.55\n",
       "0.60\n",
       "0.65\n",
       "0.70\n",
       "0.75\n",
       "0.80\n",
       "Accuracy On Examples With Label \"Significantly Better\"\n",
       "7b\n",
       "13b\n",
       "70b\n",
       "GPT4\n",
       "OpenAssistant\n",
       "Figure 6: Scaling trends for the reward model. More data and a larger-size model generally improve\n",
       "accuracy, and it appears that our models have not yet saturated from learning on the training data.\n",
       "The fact that helpfulness and safety performed the best on their own domain is potentially due to the tension\n",
       "between the two objectives (i.e., being as helpful as possible versus refusing unsafe prompts when necessary),\n",
       "which may confuse the reward model during training. In order for a single model to perform well on both\n",
       "dimensions, it needs to not only learn to select the better response given a prompt but also to distinguish\n",
       "adversarial prompts from safe ones. As a result, optimizing two separate models eases the reward modeling\n",
       "task. More detailed analysis on this tension between safety and helpfulness can be found in Appendix A.4.1.\n",
       "When we group the scores by preference rating in Table 8, we can see that the accuracy is superior for the\n",
       "“significantly better” test set and degrades gradually as comparison pairs become more similar (e.g., “slightly\n",
       "better”). It is expected that learning to model human preferences becomes challenging when deciding\n",
       "between two similar model responses, due to annotator subjectivity and their reliance on nuanced details\n",
       "that may differentiate responses. We emphasize that the accuracy on more distinct responses matters the\n",
       "most to improve Llama 2-Chat performance. The human preference annotation agreement rate is also higher\n",
       "on more distinct responses than similar pairs.\n",
       "Scaling Trends.\n",
       "We study the scaling trends in terms of data and model size for the reward model, fine-\n",
       "tuning different model s...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node 5]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 981d713a-86c9-4f5d-97d3-452fcd2e736f<br>**Similarity:** 0.338366389<br>**Text:** Figure 1: Helpfulness human evaluation results for Llama\n",
       "2-Chat compared to other open-source and closed-source\n",
       "models. Human raters compared model generations on ~4k\n",
       "prompts consisting of both single and multi-turn prompts.\n",
       "The 95% confidence intervals for this evaluation are between\n",
       "1% and 2%. More details in Section 3.4.2. While reviewing\n",
       "these results, it is important to note that human evaluations\n",
       "can be noisy due to limitations of the prompt set, subjectivity\n",
       "of the review guidelines, subjectivity of individual raters,\n",
       "and the inherent difficulty of comparing generations.\n",
       "Figure 2: Win-rate % for helpfulness and\n",
       "safety between commercial-licensed base-\n",
       "lines and Llama 2-Chat, according to GPT-\n",
       "4. To complement the human evaluation, we\n",
       "used a more capable model, not subject to\n",
       "our own guidance. Green area indicates our\n",
       "model is better according to GPT-4. To remove\n",
       "ties, we used win/(win + loss). The orders in\n",
       "which the model responses are presented to\n",
       "GPT-4 are randomly swapped to alleviate bias.\n",
       "1\n",
       "Introduction\n",
       "Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\n",
       "complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\n",
       "domains such as programming and creative writing. They enable interaction with humans through intuitive\n",
       "chat interfaces, which has led to rapid and widespread adoption among the general public.\n",
       "The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\n",
       "methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\n",
       "followed by alignment with human preferences via techniques such as Reinforcement Learning with Human\n",
       "Feedback (RLHF). Although the training methodology is simple, high computational requirements have\n",
       "limited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n",
       "(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RLHF results were evaluated using both model-based and human-based evaluation. Model-based evaluation was used to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5. Human evaluation was used to measure the robustness of the reward model, with three annotators judging the quality of the answers based on a 7-point Likert scale. Additionally, a more general reward was used to ensure the measure wouldn't diverge from human preferences. We collected a large dataset of over 1 million binary comparisons based on humans applying our specified guidelines, which we refer to as Meta reward modeling data. The reward model takes a model response and its corresponding prompt (including contexts from previous turns) as inputs and outputs a scalar score to indicate the quality (e.g., helpfulness and safety) of the model generation. Leveraging such response scores as rewards, we can optimize Llama 2-Chat during RLHF for better human preference alignment and improved helpfulness and safety. Our tuning process revealed several interesting results, such as Llama 2-Chat’s abilities to temporally organize its knowledge, or to call APIs for external tools. We have observed an intriguing phenomenon related to RLHF, a feature not previously reported to the best of our knowledge: the dynamic re-scaling of temperature contingent upon the context. As indicated in Figure 8, the temperature appears to be influenced by RLHF. Yet, intriguingly, our findings also revealed that the shifts are not uniformly applied across all prompts, as shown in Figure 21. For instance, when it comes to prompts associated with creativity, such as “Write a poem,” an increase in temperature continues to generate diversity across our various RLHF iterations. This can be observed in the Self-BLEU slope, which mirrors a pattern comparable to that of the SFT model. On the other hand, for prompts based on factual information, such as “What is the capital of ?” the Self-BLEU slope diminishes over time. This pattern suggests that despite the rising temperature, the model learns to consistently provide the same response to factual prompts. \n",
      "\n",
      "We additionally used human evaluation to compare the Llama 2-Chat models to open-source models (Falcon, MPT MosaicML NLP Team et al. (2023), Vicuna Chiang et al. (2023), as well as closed-source models (Chat-GPT (OpenAI, 2023) and PaLM Anil et al. (2023)) on over 4, 000 single and multi-turn prompts. For ChatGPT, we use gpt-3.5-turbo-0301 model in all generations. For PaLM, we use the chat-bison-001 model in all generations. The final prompt count for human evaluations for each model is shown in Table 32. As shown in Figure 12, Llama 2-Chat models outperform open-source models by a significant margin on both single turn and multi-turn prompts. \n",
      "\n",
      "We also studied the scaling trends in terms of data and model size for the reward model, fine-tuning different model sizes on an increasing amount of the reward model data collected each week (see the details on volume per batch in Table 26). Figure 6 reports these trends, showing the expected result that larger models obtain higher performance for a similar volume of data. More importantly, the scaling performance has not yet plateaued given the existing volume of data annotation used for training, a signal that there is room for more improvement with more annotations. We note that reward model accuracy is one of the most important proxies for the final performance of Llama 2-Chat. While best practices for comprehensively evaluating a generative model is an open research question, the ranking task of the reward has no ambiguity. Therefore, everything else being equal, an improvement of the reward model can be directly translated into an improvement for Llama 2-Chat.\n",
      "\n",
      "When we group the scores by preference rating in Table 8, we can see that the accuracy is superior for the “significantly better” test set and degrades gradually as comparison pairs become more similar (e.g., “slightly better”). It is expected that learning to model human preferences becomes challenging when deciding between two similar model responses, due to annotator subjectivity and their reliance on nuanced details that may differentiate responses. We emphasize that the accuracy on more distinct responses matters the most to improve Llama 2-Chat performance. The human preference annotation agreement rate is also higher on more distinct responses than similar pairs.\n",
      "\n",
      "Finally, we explored RLHF fine-tuning with two main algorithms: Proximal Policy Optimization (PPO) (Schulman et al., 2017), the standard in RLHF literature, and Rejection Sampling fine-tuning. We sample K outputs from the model and select the best candidate with our reward, consistent with Bai et al. (2022b). The same re-ranking strategy for LLMs was also proposed in Deng et al. (2019), where the reward is seen as an energy function. Here, we go one step further, and use the selected outputs for a gradient update. For each prompt, the sample obtaining the highest reward is used for the gradient update. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 2). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs.\n"
     ]
    }
   ],
   "source": [
    "response, fmt_prompts = generate_response_cr(\n",
    "    retrieved_nodes, query_str, qa_prompt, refine_prompt, llm\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nRLHF results were evaluated using both model-based and human-based evaluation. Model-based evaluation was used to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5. Human evaluation was used to measure the robustness of the reward model, with three annotators judging the quality of the answers based on a 7-point Likert scale. Additionally, a more general reward was used to ensure the measure wouldn't diverge from human preferences. We collected a large dataset of over 1 million binary comparisons based on humans applying our specified guidelines, which we refer to as Meta reward modeling data. The reward model takes a model response and its corresponding prompt (including contexts from previous turns) as inputs and outputs a scalar score to indicate the quality (e.g., helpfulness and safety) of the model generation. Leveraging such response scores as rewards, we can optimize Llama 2-Chat during RLHF for better human preference alignment and improved helpfulness and safety. Our tuning process revealed several interesting results, such as Llama 2-Chat’s abilities to temporally organize its knowledge, or to call APIs for external tools. We have observed an intriguing phenomenon related to RLHF, a feature not previously reported to the best of our knowledge: the dynamic re-scaling of temperature contingent upon the context. As indicated in Figure 8, the temperature appears to be influenced by RLHF. Yet, intriguingly, our findings also revealed that the shifts are not uniformly applied across all prompts, as shown in Figure 21. For instance, when it comes to prompts associated with creativity, such as “Write a poem,” an increase in temperature continues to generate diversity across our various RLHF iterations. This can be observed in the Self-BLEU slope, which mirrors a pattern comparable to that of the SFT model. On the other hand, for prompts based on factual information, such as “What is the capital of ?” the Self-BLEU slope diminishes over time. This pattern suggests that despite the rising temperature, the model learns to consistently provide the same response to factual prompts. \\n\\nWe additionally used human evaluation to compare the Llama 2-Chat models to open-source models (Falcon, MPT MosaicML NLP Team et al. (2023), Vicuna Chiang et al. (2023), as well as closed-source models (Chat-GPT (OpenAI, 2023) and PaLM Anil et al. (2023)) on over 4, 000 single and multi-turn prompts. For ChatGPT, we use gpt-3.5-turbo-0301 model in all generations. For PaLM, we use the chat-bison-001 model in all generations. The final prompt count for human evaluations for each model is shown in Table 32. As shown in Figure 12, Llama 2-Chat models outperform open-source models by a significant margin on both single turn and multi-turn prompts. \\n\\nWe also studied the scaling trends in terms of data and model size for the reward model, fine-tuning different model sizes on an increasing amount of the reward model data collected each week (see the details on volume per batch in Table 26). Figure 6 reports these trends, showing the expected result that larger models obtain higher performance for a similar volume of data. More importantly, the scaling performance has not yet plateaued given the existing volume of data annotation used for training, a signal that there is room for more improvement with more annotations. We note that reward model accuracy is one of the most important proxies for the final performance of Llama 2-Chat. While best practices for comprehensively evaluating a generative model is an open research question, the ranking task of the reward has no ambiguity. Therefore, everything else being equal, an improvement of the reward model can be directly translated into an improvement for Llama 2-Chat.\\n\\nWhen we group the scores by preference rating in Table 8, we can see that the accuracy is superior for the “significantly better” test set and degrades gradually as comparison pairs become more similar (e.g., “slightly better”). It is expected that learning to model human preferences becomes challenging when deciding between two similar model responses, due to annotator subjectivity and their reliance on nuanced details that may differentiate responses. We emphasize that the accuracy on more distinct responses matters the most to improve Llama 2-Chat performance. The human preference annotation agreement rate is also higher on more distinct responses than similar pairs.\\n\\nFinally, we explored RLHF fine-tuning with two main algorithms: Proximal Policy Optimization (PPO) (Schulman et al., 2017), the standard in RLHF literature, and Rejection Sampling fine-tuning. We sample K outputs from the model and select the best candidate with our reward, consistent with Bai et al. (2022b). The same re-ranking strategy for LLMs was also proposed in Deng et al. (2019), where the reward is seen as an energy function. Here, we go one step further, and use the selected outputs for a gradient update. For each prompt, the sample obtaining the highest reward is used for the gradient update. We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models. They also appear to be on par with some of the closed-source models, at least on the human evaluations we performed (see Figures 1 and 2). We have taken measures to increase the safety of these models, using safety-specific data annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally, this paper contributes a thorough description of our fine-tuning methodology and approach to improving LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and continue to improve the safety of those models, paving the way for more responsible development of LLMs.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context information is below.\n",
      "-------------------\n",
      "3.4\n",
      "RLHF Results\n",
      "3.4.1\n",
      "Model-Based Evaluation\n",
      "Evaluating LLMs is a challenging open-research problem. Human evaluation, while a gold standard, can\n",
      "be complicated by various HCI considerations (Clark et al., 2021; Gehrmann et al., 2023), and is not always\n",
      "scalable. Thus, to select the best-performing models among several ablations at each iteration from RLHF-V1\n",
      "to V5, we first observed the improvement of the rewards from the latest reward models, to save costs and\n",
      "increase iteration speed. We later validated major model versions with human evaluations.\n",
      "How Far Can Model-Based Evaluation Go?\n",
      "To measure the robustness of our reward model, we collected\n",
      "a test set of prompts for both helpfulness and safety, and asked three annotators to judge the quality of the\n",
      "answers based on a 7-point Likert scale (the higher the better). We observe that our reward models overall\n",
      "are well calibrated with our human preference annotations, as illustrated in Figure 29 in the appendix. This\n",
      "confirms the relevance of using our reward as a point-wise metric, despite being trained with a Pairwise\n",
      "Ranking Loss.\n",
      "Still, as Goodhart’s Law states, when a measure becomes a target, it ceases to be a good measure. To ensure\n",
      "our measure won’t diverge from the human preferences, we additionally used a more general reward, trained\n",
      "17\n",
      "-------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: Can you tell me about results from RLHF using both model-based and human-based evaluation?\n",
      "Answer: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view a sample qa prompt\n",
    "print(fmt_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: Can you tell me about results from RLHF using both model-based and human-based evaluation?\n",
      "We have provided an existing answer: \n",
      "RLHF results were evaluated using both model-based and human-based evaluation. Model-based evaluation was used to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5. Human evaluation was used to measure the robustness of the reward model, with three annotators judging the quality of the answers based on a 7-point Likert scale. Additionally, a more general reward was used to ensure the measure wouldn't diverge from human preferences. We collected a large dataset of over 1 million binary comparisons based on humans applying our specified guidelines, which we refer to as Meta reward modeling data. The reward model takes a model response and its corresponding prompt (including contexts from previous turns) as inputs and outputs a scalar score to indicate the quality (e.g., helpfulness and safety) of the model generation. Leveraging such response scores as rewards, we can optimize Llama 2-Chat during RLHF for better human preference alignment and improved helpfulness and safety. Our tuning process revealed several interesting results, such as Llama 2-Chat’s abilities to temporally organize its knowledge, or to call APIs for external tools. We have observed an intriguing phenomenon related to RLHF, a feature not previously reported to the best of our knowledge: the dynamic re-scaling of temperature contingent upon the context. As indicated in Figure 8, the temperature appears to be influenced by RLHF. Yet, intriguingly, our findings also revealed that the shifts are not uniformly applied across all prompts, as shown in Figure 21. For instance, when it comes to prompts associated with creativity, such as “Write a poem,” an increase in temperature continues to generate diversity across our various RLHF iterations. This can be observed in the Self-BLEU slope, which mirrors a pattern comparable to that of the SFT model. On the other hand, for prompts based on factual information, such as “What is the capital of ?” the Self-BLEU slope diminishes over time. This pattern suggests that despite the rising temperature, the model learns to consistently provide the same response to factual prompts. \n",
      "\n",
      "We additionally used human evaluation to compare the Llama 2-Chat models to open-source models (Falcon, MPT MosaicML NLP Team et al. (2023), Vicuna Chiang et al. (2023), as well as closed-source models (Chat-GPT (OpenAI, 2023) and PaLM Anil et al. (2023)) on over 4, 000 single and multi-turn prompts. For ChatGPT, we use gpt-3.5-turbo-0301 model in all generations. For PaLM, we use the chat-bison-001 model in all generations. The final prompt count for human evaluations for each model is shown in Table 32. As shown in Figure 12, Llama 2-Chat models outperform open-source models by a significant margin on both single turn and multi-turn prompts. \n",
      "\n",
      "We also studied the scaling trends in terms of data and model size for the reward model, fine-tuning different model sizes on an increasing amount of the reward model data collected each week (see the details on volume per batch in Table 26). Figure 6 reports these trends, showing the expected result that larger models obtain higher performance for a similar volume of data. More importantly, the scaling performance has not yet plateaued given the existing volume of data annotation used for training, a signal that there is room for more improvement with more annotations. We note that reward model accuracy is one of the most important proxies for the final performance of Llama 2-Chat. While best practices for comprehensively evaluating a generative model is an open research question, the ranking task of the reward has no ambiguity. Therefore, everything else being equal, an improvement of the reward model can be directly translated into an improvement for Llama 2-Chat.\n",
      "\n",
      "When we group the scores by preference rating in Table 8, we can see that the accuracy is superior for the “significantly better” test set and degrades gradually as comparison pairs become more similar (e.g., “slightly better”). It is expected that learning to model human preferences becomes challenging when deciding between two similar model responses, due to annotator subjectivity and their reliance on nuanced details that may differentiate responses. We emphasize that the accuracy on more distinct responses matters the most to improve Llama 2-Chat performance. The human preference annotation agreement rate is also higher on more distinct responses than similar pairs.\n",
      "\n",
      "Finally, we explored RLHF fine-tuning with two main algorithms: Proximal Policy Optimization (PPO) (Schulman et al., 2017), the standard in RLHF literature, and Rejection Sampling fine-tuning. We sample K outputs from the model and select the best candidate with our reward, consistent with Bai et al. (2022b). The same re-ranking strategy for LLMs was also proposed in Deng et al. (2019), where the reward is seen as an energy function. Here, we go one step further, and use the selected outputs for a gradient update. For each prompt, the sample obtaining the highest reward is used for the gradient update.\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "-------------------\n",
      "Figure 1: Helpfulness human evaluation results for Llama\n",
      "2-Chat compared to other open-source and closed-source\n",
      "models. Human raters compared model generations on ~4k\n",
      "prompts consisting of both single and multi-turn prompts.\n",
      "The 95% confidence intervals for this evaluation are between\n",
      "1% and 2%. More details in Section 3.4.2. While reviewing\n",
      "these results, it is important to note that human evaluations\n",
      "can be noisy due to limitations of the prompt set, subjectivity\n",
      "of the review guidelines, subjectivity of individual raters,\n",
      "and the inherent difficulty of comparing generations.\n",
      "Figure 2: Win-rate % for helpfulness and\n",
      "safety between commercial-licensed base-\n",
      "lines and Llama 2-Chat, according to GPT-\n",
      "4. To complement the human evaluation, we\n",
      "used a more capable model, not subject to\n",
      "our own guidance. Green area indicates our\n",
      "model is better according to GPT-4. To remove\n",
      "ties, we used win/(win + loss). The orders in\n",
      "which the model responses are presented to\n",
      "GPT-4 are randomly swapped to alleviate bias.\n",
      "1\n",
      "Introduction\n",
      "Large Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\n",
      "complex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\n",
      "domains such as programming and creative writing. They enable interaction with humans through intuitive\n",
      "chat interfaces, which has led to rapid and widespread adoption among the general public.\n",
      "The capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\n",
      "methodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\n",
      "followed by alignment with human preferences via techniques such as Reinforcement Learning with Human\n",
      "Feedback (RLHF). Although the training methodology is simple, high computational requirements have\n",
      "limited the development of LLMs to a few players. There have been public releases of pretrained LLMs\n",
      "(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\n",
      "match the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\n",
      "(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed “product” LLMs, such\n",
      "as ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\n",
      "preferences, which greatly enhances their usability and safety. This step can require significant costs in\n",
      "compute and human annotation, and is often not transparent or easily reproducible, limiting progress within\n",
      "the community to advance AI alignment research.\n",
      "In this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs, Llama 2 and\n",
      "Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "Llama 2-Chat models generally perform better than existing open-source models. They also appear to\n",
      "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
      "Figures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\n",
      "annotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\n",
      "this paper contributes a thorough description of our fine-tuning methodology and approach to improving\n",
      "LLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\n",
      "continue to improve the safety of those models, paving the way for more responsible development of LLMs.\n",
      "We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as\n",
      "the emergence of tool usage and temporal organization of knowledge.\n",
      "3\n",
      "-------------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined answer: \n"
     ]
    }
   ],
   "source": [
    "# view a sample qa prompt\n",
    "print(fmt_prompts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: This is an initial step, but obviously there are inefficiencies. One is the fact that it’s quite slow - we make sequential calls. The second piece is that each LLM call is inefficient - we are only inserting a single node, but not “stuffing” the prompt with as much context as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Hierarchical SUmmarization Strategy\n",
    "\n",
    "- Generate an answer for each node independently then hierarchically combine the answers.\n",
    "- This \"combine\" step can happen once, or for maxiumum generality can happen recursively until there is one 'root' node\n",
    "  - That root node is then returned as the final answer\n",
    "- We will implemenmt this approach below with a fixed number of children of 10; i.e. combine 10 nodes at a time\n",
    "\n",
    "NOTE: In LlamaIndex this is `tree_summarize` and in LangChain this is `map-reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33371.666666666664"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean([len(n.get_content()) for n in retrieved_nodes]) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results from RLHF using both model-based and human-based evaluation have yielded positive results. Model-based evaluation has shown that Llama 2-Chat has improved in terms of helpfulness and safety, while human-based evaluation has shown that the reward model has been able to accurately learn patterns in the preferences of the human annotators and automate preference decisions. Additionally, the collected preference data has been compared to existing open-source datasets and has been found to feature more conversation turns and be longer, on average. Human evaluation results for Llama 2-Chat compared to other open-source and closed-source models were presented in Figure 1, with 95% confidence intervals between 1% and 2%. To complement the human evaluation, a more capable model, GPT-4, was used to measure win-rate % for helpfulness and safety between commercial-licensed baselines and Llama 2-Chat, as shown in Figure 2.\n"
     ]
    }
   ],
   "source": [
    "def combine_results(\n",
    "    texts,\n",
    "    query_str,\n",
    "    qa_prompt,\n",
    "    llm,\n",
    "    cur_prompt_list,\n",
    "    num_children=10,\n",
    "):\n",
    "    new_texts = []\n",
    "\n",
    "    for idx in range(0, len(texts), num_children):\n",
    "        text_batch = texts[idx : idx + num_children]\n",
    "        context_str = \"\\n\\n\".join([t for t in text_batch])\n",
    "\n",
    "        fmt_qa_prompt = qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "        combined_response = llm.complete(fmt_qa_prompt)\n",
    "\n",
    "        new_texts.append(str(combined_response))\n",
    "        cur_prompt_list.append(fmt_qa_prompt)\n",
    "\n",
    "    if len(new_texts) == 1:\n",
    "        return new_texts[0]\n",
    "    else:\n",
    "        print(len(cur_prompt_list))\n",
    "        return combine_results(\n",
    "            new_texts, query_str, qa_prompt, llm, num_children=num_children\n",
    "        )\n",
    "    \n",
    "\n",
    "def generate_response_hs(retrieved_nodes, query_str, qa_prompt, llm, num_children=10):\n",
    "    \"\"\"Generate a response using hierarchical summarization strategy.\n",
    "\n",
    "    Combine num_children nodes hierarchically until we get one root node.\n",
    "    \"\"\"\n",
    "\n",
    "    fmt_prompts = []\n",
    "    node_responses = []\n",
    "\n",
    "    for node in retrieved_nodes:\n",
    "        context_str = node.get_content()\n",
    "\n",
    "        fmt_qa_prompt = qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "        node_response = llm.complete(fmt_qa_prompt)\n",
    "\n",
    "        node_responses.append(node_response)\n",
    "        fmt_prompts.append(fmt_qa_prompt)\n",
    "\n",
    "    response_txt = combine_results(\n",
    "        [str(r) for r in node_responses],\n",
    "        query_str,\n",
    "        qa_prompt,\n",
    "        llm,\n",
    "        fmt_prompts,\n",
    "        num_children=num_children,\n",
    "    )\n",
    "\n",
    "    return response_txt, fmt_prompts\n",
    "\n",
    "response, fmt_prompts = generate_response_hs(retrieved_nodes, query_str, qa_prompt, llm)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the above section, there are inefficiencies. We are still generating an answer for each node independently that we can try to optimize away.\n",
    "\n",
    "Our ResponseSynthesizer module handles this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [Optional] Creaet a async version of hierarchical summarization\n",
    "\n",
    "- A pro of hierarchical summarization is that LLM calls can be parallelized, speeding up response synthesis\n",
    "- Here, we use `asyncio.gather` to execute coroutines (LLM calls) for each Node concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def acombine_results(\n",
    "    texts,\n",
    "    query_str,\n",
    "    qa_prompt,\n",
    "    llm,\n",
    "    cur_prompt_list,\n",
    "    num_children=10,\n",
    "):\n",
    "    fmt_prompts = []\n",
    "\n",
    "    for idx in range(0, len(texts), num_children):\n",
    "        text_batch = texts[idx : idx + num_children]\n",
    "\n",
    "        content_str = \"\\n\\n\".join([t for t in text_batch])\n",
    "        fmt_qa_prompt = qa_prompt.format(context_str=content_str, query_str=query_str)\n",
    "\n",
    "        fmt_prompts.append(fmt_qa_prompt)\n",
    "        cur_prompt_list.append(fmt_qa_prompt)\n",
    "\n",
    "    # generate completions asynchronously\n",
    "    tasks = [llm.acomplete(p) for p in fmt_prompts]\n",
    "    combined_responses = await asyncio.gather(*tasks)  # combine tasks and run them\n",
    "    new_texts = [str(r) for r in combined_responses]\n",
    "\n",
    "    if len(new_texts) == 1:\n",
    "        return new_texts[0]\n",
    "    else:\n",
    "        return await acombine_results(\n",
    "            new_texts, query_str, qa_prompt, llm, num_children=num_children\n",
    "        )\n",
    "    \n",
    "async def agenerate_response_hs(\n",
    "    retrieved_nodes,\n",
    "    query_str,\n",
    "    qa_prompt,\n",
    "    llm,\n",
    "    num_children=10\n",
    "):\n",
    "    \"\"\"Generate a response using hierarchical summarization strategy.\n",
    "\n",
    "    Combine num_children nodes hierarchically until we get one root node.\n",
    "    \"\"\"\n",
    "\n",
    "    fmt_prompts = []\n",
    "    node_responses = []\n",
    "    for node in retrieved_nodes:\n",
    "        context_str = node.get_content()\n",
    "\n",
    "        fmt_qa_prompt = qa_prompt.format(context_str=context_str, query_str=query_str)\n",
    "        fmt_prompts.append(fmt_qa_prompt)\n",
    "    \n",
    "    tasks = [llm.acomplete(p) for p in fmt_prompts]\n",
    "    node_responses = await asyncio.gather(*tasks)  # combine tasks and run them\n",
    "\n",
    "    response_txt = combine_results(\n",
    "        [str(r) for r in node_responses],\n",
    "        query_str,\n",
    "        qa_prompt,\n",
    "        llm,\n",
    "        fmt_prompts,\n",
    "        num_children=num_children\n",
    "    )\n",
    "\n",
    "    return response_txt, fmt_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results from RLHF using both model-based and human-based evaluation have been positive. Model-based evaluation was used to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5. Human evaluation was used to measure the robustness of the reward model, with three annotators judging the quality of the answers based on a 7-point Likert scale. The results showed that Llama 2-Chat outperformed the other models by a significant margin on both single turn and multi-turn prompts. Additionally, the scaling performance had not yet plateaued given the existing volume of data annotation used for training, indicating that there is room for more improvement with more annotations. Proximal Policy Optimization (PPO) and Rejection Sampling fine-tuning were used to explore RLHF fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "response, fmt_prompts = await agenerate_response_hs(\n",
    "    retrieved_nodes, query_str, qa_prompt, llm\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers import BaseRetriever\n",
    "from llama_index.llms.base import LLM\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Response:\n",
    "    response: str\n",
    "    source_nodes: Optional[List] = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.response\n",
    "\n",
    "\n",
    "class MyQueryEngine:\n",
    "    \"\"\"My query engine.\n",
    "\n",
    "    Uses the tree summarize response synthesis module by default.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        retriever: BaseRetriever,\n",
    "        qa_prompt: PromptTemplate,\n",
    "        llm: LLM,\n",
    "        num_children=10,\n",
    "    ) -> None:\n",
    "        self._retriever = retriever\n",
    "        self._qa_prompt = qa_prompt\n",
    "        self._llm = llm\n",
    "        self._num_children = num_children\n",
    "\n",
    "    def query(self, query_str: str):\n",
    "        retrieved_nodes = self._retriever.retrieve(query_str)\n",
    "        response_txt, _ = generate_response_hs(\n",
    "            retrieved_nodes,\n",
    "            query_str,\n",
    "            self._qa_prompt,\n",
    "            self._llm,\n",
    "            num_children=self._num_children,\n",
    "        )\n",
    "        response = Response(response_txt, source_nodes=retrieved_nodes)\n",
    "        return response\n",
    "\n",
    "    async def aquery(self, query_str: str):\n",
    "        retrieved_nodes = await self._retriever.aretrieve(query_str)\n",
    "        response_txt, _ = await agenerate_response_hs(\n",
    "            retrieved_nodes,\n",
    "            query_str,\n",
    "            self._qa_prompt,\n",
    "            self._llm,\n",
    "            num_children=self._num_children,\n",
    "        )\n",
    "        response = Response(response_txt, source_nodes=retrieved_nodes)\n",
    "        return response\n",
    "\n",
    "query_engine = MyQueryEngine(retriever, qa_prompt, llm, num_children=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results from RLHF using both model-based and human-based evaluation have been positive. Model-based evaluation was used to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5. Human evaluation was used to measure the robustness of the reward model, with three annotators judging the quality of the answers based on a 7-point Likert scale. The results showed that Llama 2-Chat outperformed the other models by a significant margin on both single turn and multi-turn prompts. Additionally, a more general reward was trained to ensure the measure wouldn't diverge from the human preferences. The human preference data collected has enabled us to train a reward model which can automate preference decisions. This reward model has been used to optimize Llama 2-Chat during RLHF, resulting in better human preference alignment and improved helpfulness and safety.\n",
      "The results from RLHF using both model-based and human-based evaluation have yielded positive results. Model-based evaluation has shown that Llama 2-Chat has improved in terms of helpfulness and safety, while human-based evaluation has shown that the reward model has been able to accurately learn patterns in the preferences of the human annotators and automate preference decisions. Additionally, the collected preference data has been compared to existing open-source datasets, and has been found to feature more conversation turns and be longer, on average. The results from RLHF using model-based evaluation showed that Llama 2-Chat outperformed open-source models by a significant margin on both single turn and multi-turn prompts. For human-based evaluation, Llama 2-Chat was compared to open-source models (Falcon, MPT MosaicML NLP Team et al. (2023), Vicuna Chiang et al. (2023), as well as closed-source models (Chat-GPT (OpenAI, 2023) and PaLM Anil et al. (2023)) on over 4,000 single and multi-turn prompts. The results showed that Llama 2-Chat outperformed the other models by a significant margin on both single turn and multi-turn prompts.\n",
      "The results from RLHF using both model-based and human-based evaluation have been positive. Model-based evaluation was used to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5. Human evaluation was used to measure the robustness of the reward model, and was done using a 7-point Likert scale. The results showed that Llama 2-Chat outperformed the other models by a significant margin on both single turn and multi-turn prompts. Additionally, the accuracy on more distinct responses was higher than on similar pairs, indicating that learning to model human preferences becomes challenging when deciding between two similar model responses. Furthermore, the scaling performance had not yet plateaued given the existing volume of data annotation used for training, suggesting that there is room for more improvement with more annotations.\n",
      "The results from RLHF using both model-based and human-based evaluation showed that Llama 2-Chat outperformed open-source models by a significant margin on both single turn and multi-turn prompts. For human-based evaluation, Llama 2-Chat was compared to open-source models and closed-source models on over 4,000 single and multi-turn prompts. The results showed that Llama 2-Chat outperformed the other models by a significant margin on both single turn and multi-turn prompts. Additionally, larger models generally obtained higher performance for a similar volume of data, and the human preference annotation agreement rate was also higher on more distinct responses. Proximal Policy Optimization (PPO) and Rejection Sampling fine-tuning were the two main algorithms used for RLHF fine-tuning.\n",
      "The results from RLHF using both model-based and human-based evaluation showed that Llama 2-Chat outperformed open-source models by a significant margin on both single turn and multi-turn prompts. For human-based evaluation, Llama 2-Chat was compared to open-source models (Falcon, MPT MosaicML NLP Team et al. (2023), Vicuna Chiang et al. (2023), as well as closed-source models (Chat-GPT (OpenAI, 2023) and PaLM Anil et al. (2023)) on over 4,000 single and multi-turn prompts. The results showed that Llama 2-Chat outperformed the other models by a significant margin on both single turn and multi-turn prompts. Additionally, the accuracy on more distinct responses was higher than on similar pairs, indicating that learning to model human preferences becomes challenging when deciding between two similar model responses. Furthermore, the scaling performance had not yet plateaued given the existing volume of data annotation used for training, suggesting that there is room for more improvement with more annotations.\n",
      "The results from RLHF using both model-based and human-based evaluation have been positive. Model-based evaluation was used to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5. Human evaluation was used to measure the robustness of the reward model, with three annotators judging the quality of the answers based on a 7-point Likert scale. The results showed that Llama 2-Chat outperformed the other models by a significant margin on both single turn and multi-turn prompts. Additionally, the human preference annotation agreement rate is also higher on more distinct responses than similar pairs. The results of this evaluation were presented in Figure 1, which showed that Llama 2-Chat performed better than other open-source and closed-source models on ~4k prompts consisting of both single and multi-turn prompts.\n",
      "The results from RLHF using both model-based and human-based evaluation showed that Llama 2-Chat outperformed open-source models by a significant margin on both single turn and multi-turn prompts. For human-based evaluation, Llama 2-Chat models were compared to open-source models (Falcon, MPT MosaicML NLP Team et al. (2023), Vicuna Chiang et al. (2023), as well as closed-source models (Chat-GPT (OpenAI, 2023) and PaLM Anil et al. (2023)) on over 4,000 single and multi-turn prompts. The results showed that Llama 2-Chat outperformed the other models by a significant margin on both single turn and multi-turn prompts. Additionally, larger models generally obtained higher performance for a similar volume of data, and the accuracy on more distinct responses was higher than on similar pairs.\n",
      "The results from RLHF using both model-based and human-based evaluation have been positive. Model-based evaluation was used to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5. Human evaluation was used to measure the robustness of the reward model, with three annotators judging the quality of the answers based on a 7-point Likert scale. The results from RLHF using model-based evaluation showed that Llama 2-Chat outperformed open-source models by a significant margin on both single turn and multi-turn prompts. For human-based evaluation, Llama 2-Chat models were compared to open-source models and closed-source models on over 4,000 single and multi-turn prompts. The results showed that Llama 2-Chat outperformed the other models by a significant margin on both single turn and multi-turn prompts. Additionally, the accuracy on more distinct responses matters the most to improve Llama 2-Chat performance. The human preference annotation agreement rate is also higher on more distinct responses than similar pairs. The results of this evaluation are shown in Figure 1, which shows the helpfulness human evaluation results for Llama 2-Chat compared to other open-source and closed-source models. Figure 2 shows the win-rate % for helpfulness and safety between commercial-licensed baselines and Llama 2-Chat, according to GPT-4.\n",
      "22 s ± 3.73 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results from RLHF using both model-based and human-based evaluation have been positive. Model-based evaluation was used to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5. Human evaluation was used to measure the robustness of the reward model, and three annotators were asked to judge the quality of the answers based on a 7-point Likert scale. The results showed that Llama 2-Chat outperformed the other models by a significant margin on both single turn and multi-turn prompts. Additionally, the human preference data collected through the binary comparison protocol has enabled us to maximize the diversity of collected prompts and train a reward model that can learn patterns in the preferences of the human annotators. This reward model has been used to optimize Llama 2-Chat during RLHF, resulting in better human preference alignment and improved helpfulness and safety.\n"
     ]
    }
   ],
   "source": [
    "response = await query_engine.aquery(query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-rag-from-scratch-xZSvx2P--py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
