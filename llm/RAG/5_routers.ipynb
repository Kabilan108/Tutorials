{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this notebook, we'll build an LLM powered router to route queires to submodules\n",
    "\n",
    "- Steps:\n",
    "    - Crafting an initial prompt to select a set of choices\n",
    "    - Enforcing structured output (for text completion endpoints)\n",
    "    - Try integrating with a native function calling endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Basic Router Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import PromptTemplate\n",
    "\n",
    "choices = [\n",
    "    \"Useful for questions related to apples\",\n",
    "    \"Useful for questions related to oranges\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_choice_str(choices):\n",
    "    choices_str = \"\\n\\n\".join([f\"{idx+1}. {c}\" for idx, c in enumerate(choices)])\n",
    "    return choices_str\n",
    "\n",
    "\n",
    "choices_str = get_choice_str(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt0 = PromptTemplate(\n",
    "    \"Some choices are given below. It is provided in a numbered \"\n",
    "    \"list (1 to {num_choices}), \"\n",
    "    \"where each item in the list corresponds to a summary.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_list}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Using only the choices above and not prior knowledge, return the top choices \"\n",
    "    \"(no more than {max_outputs}, but only select what is needed) that \"\n",
    "    \"are most relevant to the question: '{query_str}'\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "def get_formatted_prompt(query_str):\n",
    "    fmt_prompt = router_prompt0.format(\n",
    "        num_choices=len(choices),\n",
    "        max_outputs=2,\n",
    "        context_list=choices_str,\n",
    "        query_str=query_str,\n",
    "    )\n",
    "    return fmt_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Useful for questions related to apples\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Can you tell me more about the amount of Vitamin C in apples\"\n",
    "fmt_prompt = get_formatted_prompt(query_str)\n",
    "\n",
    "response = llm.complete(fmt_prompt)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Useful for questions related to oranges\n"
     ]
    }
   ],
   "source": [
    "query_str = \"What are the health benefits of eating orange peels?\"\n",
    "fmt_prompt = get_formatted_prompt(query_str)\n",
    "\n",
    "response = llm.complete(fmt_prompt)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Useful for questions related to apples\n",
      "2. Useful for questions related to oranges\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Can you tell me more about the amount of Vitamin C in apples and oranges.\"\n",
    "fmt_prompt = get_formatted_prompt(query_str)\n",
    "\n",
    "response = llm.complete(fmt_prompt)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Enforce Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"description\": \"Class for storing the answer and its metadata\",\n",
      "  \"properties\": {\n",
      "    \"choice\": {\n",
      "      \"title\": \"Choice\",\n",
      "      \"type\": \"string\"\n",
      "    },\n",
      "    \"reason\": {\n",
      "      \"title\": \"Reason\",\n",
      "      \"type\": \"string\"\n",
      "    }\n",
      "  },\n",
      "  \"required\": [\n",
      "    \"choice\",\n",
      "    \"reason\"\n",
      "  ],\n",
      "  \"title\": \"Answer\",\n",
      "  \"type\": \"object\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import fields\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"Class for storing the answer and its metadata\"\"\"\n",
    "    choice: str\n",
    "    reason: str\n",
    "\n",
    "print(json.dumps(Answer.model_json_schema(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.types import BaseOutputParser\n",
    "\n",
    "FORMAT_STR = \"\"\"\\\n",
    "The output should be formatted as a JSON instance that conforms to \n",
    "the JSON schema below. \n",
    "\n",
    "Here is the output schema:\n",
    "{\n",
    "  \"description\": \"Class for storing the answer and its metadata\",\n",
    "  \"properties\": {\n",
    "    \"choice\": {\n",
    "      \"title\": \"Choice\",\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"reason\": {\n",
    "      \"title\": \"Reason\",\n",
    "      \"type\": \"string\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\n",
    "    \"choice\",\n",
    "    \"reason\"\n",
    "  ],\n",
    "  \"title\": \"Answer\",\n",
    "  \"type\": \"object\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to excape {} in f-string for PromptTemplate\n",
    "def _escape_curly_braces(input_string: str) -> str:\n",
    "    # Replace '{' with '{{' and '}' with '}}'\n",
    "    escaped = input_string.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    return escaped\n",
    "\n",
    "\n",
    "# Define parsing function to extract JSON from LLM response by searching for square brackets\n",
    "def _marshal_output_to_json(output: str) -> str:\n",
    "    output = output.strip()\n",
    "    left = output.find(\"[\")\n",
    "    right = output.find(\"]\")\n",
    "    output = output[left : right + 1]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class RouterOutputParser(BaseOutputParser):\n",
    "    def parse(self, output: str) -> List[Answer]:\n",
    "        \"\"\"Parse String\"\"\"\n",
    "\n",
    "        json_output = _marshal_output_to_json(output)\n",
    "        json_dicts = json.loads(json_output)\n",
    "        answers = [Answer.from_dict(json_dict) for json_dict in json_dicts]\n",
    "        return answers\n",
    "    \n",
    "    def format(self, prompt_template: str) -> str:\n",
    "        return prompt_template + \"\\n\\n\" + _escape_curly_braces(FORMAT_STR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = RouterOutputParser()\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def route_query(query_str: str, choices: List[str], output_parser: RouterOutputParser):\n",
    "    choices_str\n",
    "\n",
    "    fmt_base_prompt = router_prompt0.format(\n",
    "        num_choices=len(choices),\n",
    "        max_outputs=len(choices),\n",
    "        context_list=choices_str,\n",
    "        query_str=query_str,\n",
    "    )\n",
    "    fmt_json_prompt = output_parser.format(fmt_base_prompt)\n",
    "\n",
    "    raw_output = llm.complete(fmt_json_prompt)\n",
    "    parsed = output_parser.parse(str(raw_output))\n",
    "\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Perform routing using a Function calling endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'Answer': {'description': 'Represents a single choice with a reason',\n",
       "   'properties': {'choice': {'title': 'Choice', 'type': 'integer'},\n",
       "    'reason': {'title': 'Reason', 'type': 'string'}},\n",
       "   'required': ['choice', 'reason'],\n",
       "   'title': 'Answer',\n",
       "   'type': 'object'}},\n",
       " 'description': 'Represents a list of answeres',\n",
       " 'properties': {'answers': {'items': {'$ref': '#/$defs/Answer'},\n",
       "   'title': 'Answers',\n",
       "   'type': 'array'}},\n",
       " 'required': ['answers'],\n",
       " 'title': 'Answers',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"Represents a single choice with a reason\"\"\"\n",
    "    choice: int\n",
    "    reason: str\n",
    "\n",
    "class Answers(BaseModel):\n",
    "    \"\"\"Represents a list of answeres\"\"\"\n",
    "    answers: List[Answer]\n",
    "\n",
    "Answers.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.program import OpenAIPydanticProgram\n",
    "\n",
    "router_prompt1 = router_prompt0.partial_format(\n",
    "    num_choices=len(choices),\n",
    "    max_outputs=len(choices)\n",
    ")\n",
    "\n",
    "program = OpenAIPydanticProgram.from_defaults(\n",
    "    output_cls=Answers,\n",
    "    prompt=router_prompt1,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function call: Answers with args: {\n",
      "  \"answers\": [\n",
      "    {\n",
      "      \"choice\": 2,\n",
      "      \"reason\": \"Useful for questions related to oranges\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query_str = \"What are the health benefits of eating orange peels?\"\n",
    "output = program(context_list=choices_str, query_str=query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers=[Answer(choice=2, reason='Useful for questions related to oranges')]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plug router into RAG Pipeline\n",
    "\n",
    "- Here, we’ll use the router in a RAG pipeline\n",
    "- We'll use it to dynamically decide whether to perform question-answering or summarization\n",
    "  - We can easily get a question-answering query engine using top-k retrieval through the `VectorIndex`\n",
    "  - and summarization through the `SummaryIndex`\n",
    "- Each query engine is presented as a choice to the router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "--2023-09-20 19:36:05--  https://arxiv.org/pdf/2307.09288.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 128.84.21.199\n",
      "Connecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13661300 (13M) [application/pdf]\n",
      "Saving to: ‘data/llama2.pdf’\n",
      "\n",
      "data/llama2.pdf     100%[===================>]  13.03M  1.95MB/s    in 8.6s    \n",
      "\n",
      "2023-09-20 19:36:14 (1.52 MB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load darta\n",
    "!mkdir data\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_hub.file.pymu_pdf.base import PyMuPDFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define indexes\n",
    "from llama_index import ServiceContext, VectorStoreIndex, SummaryIndex\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size=1024)\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "\n",
    "summary_index = SummaryIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "vector_query_engine = vector_index.as_query_engine()\n",
    "summary_query_engine = summary_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RouterQUeryEngine\n",
    "from llama_index.query_engine import CustomQueryEngine, BaseQueryEngine\n",
    "from llama_index.response_synthesizers import TreeSummarize\n",
    "\n",
    "\n",
    "class RouterQueryEngine(CustomQueryEngine):\n",
    "    \"\"\"Use Pydantic Program to perform routing.\"\"\"\n",
    "\n",
    "    query_engines: List[BaseQueryEngine]\n",
    "    choice_descriptions: List[str]\n",
    "    verbose: bool = False\n",
    "    router_prompt: PromptTemplate\n",
    "    llm: OpenAI\n",
    "    summarizer: TreeSummarize = Field(default_factory=TreeSummarize)\n",
    "\n",
    "    def custom_query(self, query_str):\n",
    "        \"\"\"Define the custom query\"\"\"\n",
    "\n",
    "        # Create program\n",
    "        program = OpenAIPydanticProgram.from_defaults(\n",
    "            output_cls=Answers,\n",
    "            prompt=self.router_prompt,\n",
    "            verbose=self.verbose,\n",
    "            llm=self.llm,\n",
    "        )\n",
    "\n",
    "        # Define choices\n",
    "        choices_str = get_choice_str(self.choice_descriptions)\n",
    "        output = program(context_list=choices_str, query_str=query_str)\n",
    "\n",
    "        # print choice and reason, and query underlying engine\n",
    "        if self.verbose:\n",
    "            print(f\"Selected choice(s):\")\n",
    "            for answer in output.answers:\n",
    "                print(f\"Choice: {answer.choice}, Reason: {answer.reason}\")\n",
    "\n",
    "        # submit queries for each choice (QA)\n",
    "        responses = []\n",
    "        for answer in output.answers: \n",
    "            choice_idx = answer.choice - 1\n",
    "            query_engine = self.query_engines[choice_idx]\n",
    "            response = query_engine.query(query_str)\n",
    "            responses.append(response)\n",
    "\n",
    "        if len(responses) == 1:\n",
    "            return responses[0]\n",
    "        else:\n",
    "            # if multiple choices are picked, we can pick a summarizer\n",
    "            response_strs = [str(r) for r in responses]\n",
    "            result_response = self.summarizer.get_response(query_str, response_strs)\n",
    "            return result_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseModel.validate() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/sietch/tutorials/llm/RAG/5_routers.ipynb Cell 26\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/5_routers.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m choices \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/5_routers.ipynb#X50sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mUseful for answering questions about specific sections of the Llama 2 paper\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/5_routers.ipynb#X50sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mUseful for questions that ask for a summary of the whole paper\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/5_routers.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/5_routers.ipynb#X50sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m router_query_engine \u001b[39m=\u001b[39m RouterQueryEngine(\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/5_routers.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     query_engines\u001b[39m=\u001b[39;49m[vector_query_engine, summary_query_engine],\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/5_routers.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     choice_descriptions\u001b[39m=\u001b[39;49mchoices,\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/5_routers.ipynb#X50sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/5_routers.ipynb#X50sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     router_prompt\u001b[39m=\u001b[39;49mrouter_prompt1,\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/5_routers.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     llm\u001b[39m=\u001b[39;49mOpenAI(model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-4\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/sietch/tutorials/llm/RAG/5_routers.ipynb#X50sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/llama-index-rag-from-scratch-xZSvx2P--py3.10/lib/python3.10/site-packages/pydantic/main.py:165\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    164\u001b[0m __tracebackhide__ \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m __pydantic_self__\u001b[39m.\u001b[39;49m__pydantic_validator__\u001b[39m.\u001b[39;49mvalidate_python(data, self_instance\u001b[39m=\u001b[39;49m__pydantic_self__)\n",
      "\u001b[0;31mTypeError\u001b[0m: BaseModel.validate() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "choices = [\n",
    "    \"Useful for answering questions about specific sections of the Llama 2 paper\",\n",
    "    \"Useful for questions that ask for a summary of the whole paper\",\n",
    "]\n",
    "\n",
    "router_query_engine = RouterQueryEngine(\n",
    "    query_engines=[vector_query_engine, summary_query_engine],\n",
    "    choice_descriptions=choices,\n",
    "    verbose=True,\n",
    "    router_prompt=router_prompt1,\n",
    "    llm=OpenAI(model=\"gpt-4\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-rag-from-scratch-xZSvx2P--py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
