{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, we will build a simple in-memory vector store that can store documents and metadata.\n",
    "  - It will expose a query interface that can support a variety of queries:\n",
    "    - semantic search\n",
    "    - metadata filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setup\n",
    "  -  load some documents and parse into Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n",
      "--2023-09-20 12:55:55--  https://arxiv.org/pdf/2307.09288.pdf\n",
      "Resolving arxiv.org (arxiv.org)... 128.84.21.199\n",
      "Connecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13661300 (13M) [application/pdf]\n",
      "Saving to: ‘data/llama2.pdf’\n",
      "\n",
      "data/llama2.pdf     100%[===================>]  13.03M  4.05MB/s    in 3.2s    \n",
      "\n",
      "2023-09-20 12:55:59 (4.05 MB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_hub.file.pymu_pdf.base import PyMuPDFReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=256)\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "\n",
    "embed_model = OpenAIEmbedding()\n",
    "\n",
    "# generate embeddings for each node\n",
    "for node in nodes:\n",
    "    node.embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To build out in-memory vector store we'll use a python dictionary.\n",
    "- First we will implement embedding search, and then add metadata filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interface: get, add, delete, query, persist\n",
    "from llama_index.vector_stores.types import (\n",
    "    VectorStore,\n",
    "    VectorStoreQuery,\n",
    "    VectorStoreQueryResult,\n",
    ")\n",
    "from typing import List, Any, Optional, Dict\n",
    "from llama_index.schema import TextNode, BaseNode\n",
    "import os\n",
    "\n",
    "\n",
    "class BaseVectorStore(VectorStore):\n",
    "    \"\"\"Simple custom vector store\n",
    "    Stores documents in a simple in-memory dict\n",
    "    \"\"\"\n",
    "\n",
    "    stores_text: bool = True\n",
    "\n",
    "    def get(self, text_id: str) -> List[float]:\n",
    "        \"\"\"Get embedding\"\"\"\n",
    "        pass\n",
    "\n",
    "    def add(self, nodes: List[BaseNode]) -> List[str]:\n",
    "        \"\"\"Add nodes to index\"\"\"\n",
    "        pass\n",
    "\n",
    "    def delete(self, ref_doc_id: str, **delete_kwargs: Any) -> None:\n",
    "        \"\"\"Delete nodes using with ref_doc_id\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        ref_doc_id : str\n",
    "            The doc_id of the document to delete\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def persist(self, persist_path: str, fs=None) -> None:\n",
    "        \"\"\"Persist the SipleVectorStore to a directory\n",
    "        \n",
    "        NOTE: Not implementing now\n",
    "        \"\"\"\n",
    "        pass        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at some of the classes defined here.\n",
    "\n",
    "BaseNode is simply the parent class of our core Node modules. Each Node represents a text chunk + associated metadata.\n",
    "\n",
    "We also use some lower-level constructs, for instance our VectorStoreQuery and VectorStoreQueryResult. These are just lightweight dataclass containers to represent queries and results. We look at the dataclass fields below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_embedding': typing.Optional[typing.List[float]],\n",
       " 'similarity_top_k': int,\n",
       " 'doc_ids': typing.Optional[typing.List[str]],\n",
       " 'node_ids': typing.Optional[typing.List[str]],\n",
       " 'query_str': typing.Optional[str],\n",
       " 'output_fields': typing.Optional[typing.List[str]],\n",
       " 'embedding_field': typing.Optional[str],\n",
       " 'mode': <enum 'VectorStoreQueryMode'>,\n",
       " 'alpha': typing.Optional[float],\n",
       " 'filters': typing.Optional[llama_index.vector_stores.types.MetadataFilters],\n",
       " 'mmr_threshold': typing.Optional[float],\n",
       " 'sparse_top_k': typing.Optional[int]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import fields\n",
    "\n",
    "{f.name: f.type for f in fields(VectorStoreQuery)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': typing.Optional[typing.Sequence[llama_index.schema.BaseNode]],\n",
       " 'similarities': typing.Optional[typing.List[float]],\n",
       " 'ids': typing.Optional[typing.List[str]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{f.name: f.type for f in fields(VectorStoreQueryResult)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define add, get, delete\n",
    "class VectorStore2(BaseVectorStore):\n",
    "    \"\"\"VectorStore wtih add/get/delete methods\"\"\"\n",
    "\n",
    "    stores_text: bool = True\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._node_dict: Dict[str, BaseNode] = {}\n",
    "\n",
    "    def get(self, text_id: str) -> List[float]:\n",
    "        return self._node_dict[text_id]\n",
    "    \n",
    "    def add(self, nodes: List[BaseNode]) -> List[str]:\n",
    "        for node in nodes:\n",
    "            self._node_dict[node.node_id] = node\n",
    "    \n",
    "    def delete(self, node_id: str, **delete_kwargs: Any) -> None:\n",
    "        del self._node_dict[node_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node ID: id1\n",
      "Text: hello world\n"
     ]
    }
   ],
   "source": [
    "# basic tests\n",
    "test_node = TextNode(id_=\"id1\", text=\"hello world\")\n",
    "test_node2 = TextNode(id_=\"id2\", text=\"foo bar\")\n",
    "test_nodes = [test_node, test_node2]\n",
    "\n",
    "vector_store = VectorStore2()\n",
    "vector_store.add(test_nodes)\n",
    "node = vector_store.get(\"id1\")\n",
    "print(str(node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.a. Defining `query` - semantic search\n",
    "\n",
    "- implement a basic version of top-k similarity search\n",
    "  - iterates through doc embeddings, computing cosine-similarity with the query embedding\n",
    "  - top k documents by cosine similarity are returned\n",
    "\n",
    "Cosine similarity: $\\dfrac{\\vec{d}\\vec{q}}{|\\vec{d}||\\vec{q}|}$ for every document, query embedding pair $\\vec{d}$, $\\vec{q}$\n",
    "\n",
    "NOTE: The top-k value is contained in the VectorStoreQuery container.\n",
    "\n",
    "NOTE: Similar to the above, we define another subclass just so we don’t have to reimplement the above functions (not because this is actually good code practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "\n",
    "def get_top_k_embeddings(\n",
    "    query_embedding: List[float],\n",
    "    doc_embeddings: List[List[float]],\n",
    "    doc_ids: List[str],\n",
    "    similarity_top_k: int = 5,\n",
    ") -> Tuple[List[float], List]:\n",
    "    \"\"\"Get top nodes by similarity to the query\"\"\"\n",
    "\n",
    "    # dimensions: D\n",
    "    qembed_np = np.array(query_embedding)\n",
    "\n",
    "    # dimensions: N x D\n",
    "    dembed_np = np.array(doc_embeddings)\n",
    "\n",
    "    # dimensions: N\n",
    "    dproduct_arr = np.dot(dembed_np, qembed_np)\n",
    "\n",
    "    # dimensions: N\n",
    "    norm_arr = np.linalg.norm(qembed_np) * np.linalg.norm(dembed_np, axis=1, keepdims=False)\n",
    "\n",
    "    # dimensions: N\n",
    "    cos_sim_arr = dproduct_arr / norm_arr \n",
    "\n",
    "    # Now we have the N cosine similarities for each doc\n",
    "    # sort by top k and return\n",
    "    tups = [(cos_sim_arr[i], doc_ids[i]) for i in range(len(doc_ids))]\n",
    "    sorted_tups = sorted(tups, key=lambda t: t[0], reverse=True)\n",
    "\n",
    "    sorted_tups = sorted_tups[:similarity_top_k]\n",
    "\n",
    "    result_similarities = [s for s, _ in sorted_tups]\n",
    "    result_ids = [n for _, n in sorted_tups]\n",
    "    return result_similarities, result_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "\n",
    "class VectorStore3A(VectorStore2):\n",
    "    \"\"\"Implement semantic/dense search.\"\"\"\n",
    "\n",
    "    def query(self, query: VectorStoreQuery, **kwargs: Any):\n",
    "\n",
    "        query_embedding = cast(List[float], query.query_embedding)\n",
    "        doc_embeddings = [n.embedding for n in self.node_dict.values()]\n",
    "        doc_ids = [n.node_id for n in self.node_dict.values()]\n",
    "\n",
    "        similarities, node_ids = get_top_k_embeddings(\n",
    "            query_embedding=query_embedding,\n",
    "            doc_embeddings=doc_embeddings,\n",
    "            doc_ids=doc_ids,\n",
    "            similarity_top_k=query.similarity_top_k,\n",
    "        )\n",
    "\n",
    "        result_nodes = [self.node_dict[node_id] for node_id in node_ids]\n",
    "\n",
    "        return VectorStoreQueryResult(\n",
    "            nodes=result_nodes, similarities=similarities, ids=node_ids\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.b. Supporting Metadata Filtering\n",
    "The next extension is adding metadata filter support. This means that we will first filter the candidate set with documents that pass the metadata filters, and then perform semantic querying.\n",
    "\n",
    "For simplicity we use metadata filters for exact matching with an AND condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import MetadataFilters\n",
    "from llama_index.schema import BaseNode\n",
    "from typing import cast\n",
    "\n",
    "\n",
    "def filter_nodes(nodes: List[BaseNode], filters: MetadataFilters):\n",
    "    filtered_nodes = []\n",
    "\n",
    "    for node in nodes:\n",
    "        matches = True\n",
    "        for f in filters.filters:\n",
    "            if f.key not in node.metadata:\n",
    "                matches = False\n",
    "                continue\n",
    "            if f.value != node.metadata[f.key]:\n",
    "                matches = False\n",
    "                continue\n",
    "        if matches:\n",
    "            filtered_nodes.append(node)\n",
    "\n",
    "    return filtered_nodes\n",
    "\n",
    "\n",
    "def dense_search(query: VectorStoreQuery, nodes: List[BaseNode]):\n",
    "    \"\"\"Dense search\"\"\"\n",
    "    query_embedding = cast(List[float], query.query_embedding)\n",
    "    doc_embeddings = [n.embedding for n in nodes]\n",
    "    doc_ids = [n.node_id for n in nodes]\n",
    "\n",
    "    return get_top_k_embeddings(\n",
    "        query_embedding=query_embedding,\n",
    "        doc_embeddings=doc_embeddings,\n",
    "        doc_ids=doc_ids,\n",
    "        similarity_top_k=query.similarity_top_k,\n",
    "    )\n",
    "\n",
    "\n",
    "class VectorStore3B(VectorStore2):\n",
    "    \"\"\"Implement Metadata filtering\"\"\"\n",
    "\n",
    "    def query(self, query: VectorStoreQuery, **kwargs: Any):\n",
    "\n",
    "        nodes = self._node_dict.values()\n",
    "\n",
    "        # Filter by metadata\n",
    "        if query.filters is not None:\n",
    "            nodes = filter_nodes(nodes, query.filters)\n",
    "\n",
    "        if len(nodes) == 0:\n",
    "            result_nodes, similarities, node_ids = [], [], []\n",
    "        else:\n",
    "            # semantic search\n",
    "            similarities, node_ids = dense_search(query, nodes)\n",
    "            result_nodes = [self._node_dict[node_id] for node_id in node_ids]\n",
    "\n",
    "        return VectorStoreQueryResult(\n",
    "            nodes=result_nodes, similarities=similarities, ids=node_ids\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load data into the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore3B()\n",
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Can you tell me about the key concepts for safety finetuning\"\n",
    "query_embedding = embed_model.get_query_embedding(query_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------\n",
      "[Node ID da13256d-69d7-4526-8fd0-83fc4219eaec] Similarity: 0.8353106487126585\n",
      "\n",
      "total_pages: 77\n",
      "file_path: ./data/llama2.pdf\n",
      "source: 23\n",
      "\n",
      "Specifically, we use the following techniques in safety fine-tuning:\n",
      "1. Supervised Safety Fine-Tuning: We initialize by gathering adversarial prompts and safe demonstra-\n",
      "tions that are then included in the general supervised fine-tuning process (Section 3.1). This teaches\n",
      "the model to align with our safety guidelines even before RLHF, and thus lays the foundation for\n",
      "high-quality human preference data annotation.\n",
      "2. Safety RLHF: Subsequently, we integrate safety in the general RLHF pipeline described in Sec-\n",
      "tion 3.2.2. This includes training a safety-specific reward model and gathering more challenging\n",
      "adversarial prompts for rejection sampling style fine-tuning and PPO optimization.\n",
      "3. Safety Context Distillation: Finally, we refine our RLHF pipeline with context distillation (Askell\n",
      "et al., 2021b).\n",
      "----------------\n",
      "\n",
      "\n",
      "\n",
      "----------------\n",
      "[Node ID b431dde9-67aa-4439-92e5-bb406b4dd974] Similarity: 0.8274838500363874\n",
      "\n",
      "total_pages: 77\n",
      "file_path: ./data/llama2.pdf\n",
      "source: 23\n",
      "\n",
      "Benchmarks give a summary view of model capabilities and behaviors that allow us to understand general\n",
      "patterns in the model, but they do not provide a fully comprehensive view of the impact the model may have\n",
      "on people or real-world outcomes; that would require study of end-to-end product deployments. Further\n",
      "testing and mitigation should be done to understand bias and other social issues for the specific context\n",
      "in which a system may be deployed. For this, it may be necessary to test beyond the groups available in\n",
      "the BOLD dataset (race, religion, and gender). As LLMs are integrated and deployed, we look forward to\n",
      "continuing research that will amplify their potential for positive impact on these important social issues.\n",
      "4.2\n",
      "Safety Fine-Tuning\n",
      "In this section, we describe our approach to safety fine-tuning, including safety categories, annotation\n",
      "guidelines, and the techniques we use to mitigate safety risks. We employ a process similar to the general\n",
      "fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.\n",
      "----------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_obj = VectorStoreQuery(query_embedding=query_embedding, similarity_top_k=2)\n",
    "\n",
    "query_result = vector_store.query(query_obj)\n",
    "for similarity, node in zip(query_result.similarities, query_result.nodes):\n",
    "    print(\n",
    "        \"\\n----------------\\n\"\n",
    "        f\"[Node ID {node.node_id}] Similarity: {similarity}\\n\\n\"\n",
    "        f\"{node.get_content(metadata_mode='all')}\"\n",
    "        \"\\n----------------\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------\n",
      "[Node ID a3fdf251-0340-461f-abbd-bca5a8f47a33] Similarity: 0.8185257137702916\n",
      "\n",
      "total_pages: 77\n",
      "file_path: ./data/llama2.pdf\n",
      "source: 24\n",
      "\n",
      "4.2.2\n",
      "Safety Supervised Fine-Tuning\n",
      "In accordance with the established guidelines from Section 4.2.1, we gather prompts and demonstrations\n",
      "of safe model responses from trained annotators, and use the data for supervised fine-tuning in the same\n",
      "manner as described in Section 3.1. An example can be found in Table 5.\n",
      "The annotators are instructed to initially come up with prompts that they think could potentially induce\n",
      "the model to exhibit unsafe behavior, i.e., perform red teaming, as defined by the guidelines. Subsequently,\n",
      "annotators are tasked with crafting a safe and helpful response that the model should produce.\n",
      "4.2.3\n",
      "Safety RLHF\n",
      "We observe early in the development of Llama 2-Chat that it is able to generalize from the safe demonstrations\n",
      "in supervised fine-tuning. The model quickly learns to write detailed safe responses, address safety concerns,\n",
      "explain why the topic might be sensitive, and provide additional helpful information.\n",
      "----------------\n",
      "\n",
      "\n",
      "\n",
      "----------------\n",
      "[Node ID 90246222-69bc-4fd4-90e9-d1e252b1254c] Similarity: 0.8008875944490862\n",
      "\n",
      "total_pages: 77\n",
      "file_path: ./data/llama2.pdf\n",
      "source: 24\n",
      "\n",
      "In particular, when\n",
      "the model outputs safe responses, they are often more detailed than what the average annotator writes.\n",
      "Therefore, after gathering only a few thousand supervised demonstrations, we switched entirely to RLHF to\n",
      "teach the model how to write more nuanced responses. Comprehensive tuning with RLHF has the added\n",
      "benefit that it may make the model more robust to jailbreak attempts (Bai et al., 2022a).\n",
      "We conduct RLHF by first collecting human preference data for safety similar to Section 3.2.2: annotators\n",
      "write a prompt that they believe can elicit unsafe behavior, and then compare multiple model responses to\n",
      "the prompts, selecting the response that is safest according to a set of guidelines. We then use the human\n",
      "preference data to train a safety reward model (see Section 3.2.2), and also reuse the adversarial prompts to\n",
      "sample from the model during the RLHF stage.\n",
      "Better Long-Tail Safety Robustness without Hurting Helpfulness\n",
      "Safety is inherently a long-tail problem,\n",
      "where the challenge comes from a small number of very specific cases.\n",
      "----------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query with metadata filters\n",
    "filters = MetadataFilters.from_dict({\"source\": \"24\"})\n",
    "\n",
    "query_obj = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=2, filters=filters\n",
    ")\n",
    "\n",
    "query_result = vector_store.query(query_obj)\n",
    "\n",
    "for similarity, node in zip(query_result.similarities, query_result.nodes):\n",
    "    print(\n",
    "        \"\\n----------------\\n\"\n",
    "        f\"[Node ID {node.node_id}] Similarity: {similarity}\\n\\n\"\n",
    "        f\"{node.get_content(metadata_mode='all')}\"\n",
    "        \"\\n----------------\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build a RAD system with the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key concepts for safety fine-tuning include supervised safety fine-tuning, safety RLHF (Reinforcement Learning from Human Feedback), and safety context distillation. Supervised safety fine-tuning involves gathering adversarial prompts and safe demonstrations to align the model with safety guidelines before RLHF. Safety RLHF integrates safety into the RLHF pipeline by training a safety-specific reward model and gathering more challenging adversarial prompts for fine-tuning and optimization. Finally, safety context distillation is used to refine the RLHF pipeline. These techniques aim to mitigate safety risks and ensure that the model aligns with safety guidelines.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "query_str = \"Can you tell me about the key concepts for safety finetuning\"\n",
    "\n",
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-index-rag-from-scratch-xZSvx2P--py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
